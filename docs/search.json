[
  {
    "objectID": "wildfire_risk_main_code.html#the-algeria-dataset",
    "href": "wildfire_risk_main_code.html#the-algeria-dataset",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "The Algeria Dataset:",
    "text": "The Algeria Dataset:\nThis dataset includes 244 instances that regrouped data from two regions of Algeria, namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region located in the northwest of Algeria. There are 122 instances for each region. This data was collected from June 2012 to September 2012. The dataset contains fire and non-fire events with 13 attributes that describe weather characteristics at the time of that event.\nBefore training the dataset on our machine learning models, we will clean up some of the data. To do this, we’ll convert the target label “Fire” into binary numbers. We use “one-hot encoding” to achieve this, in which each category is represented by a binary column, with a 1 indicating that the a forest fire occured given the conditions. In addition, we will remove any rows with missing data.\n\nalgeria_df['Classes'] = algeria_df['Classes'].str.strip()\nalgeria_df.drop(algeria_df[(algeria_df['Classes'] != 'fire') & (algeria_df['Classes'] != 'not fire')].index, inplace=True)\nalgeria_df['Classes'].unique()\n\nalgeria_df['Classes'].replace(('not fire', 'fire'),(0,1),inplace = True)\nalgeria_df.rename(columns = {'Classes': 'Fire'},inplace = True)\nalgeria_df['Fire'].unique()\n\nalgeria_df = algeria_df.astype(float)\nalgeria_df = pd.get_dummies(algeria_df)\nalgeria_df.dropna(axis = 0)\n\nalgeria_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      day\n      month\n      year\n      Temperature\n      RH\n      Ws\n      Rain\n      FFMC\n      DMC\n      DC\n      ISI\n      BUI\n      FWI\n      Fire\n    \n  \n  \n    \n      0\n      1.0\n      6.0\n      2012.0\n      29.0\n      57.0\n      18.0\n      0.0\n      65.7\n      3.4\n      7.6\n      1.3\n      3.4\n      0.5\n      0.0\n    \n    \n      1\n      2.0\n      6.0\n      2012.0\n      29.0\n      61.0\n      13.0\n      1.3\n      64.4\n      4.1\n      7.6\n      1.0\n      3.9\n      0.4\n      0.0\n    \n    \n      2\n      3.0\n      6.0\n      2012.0\n      26.0\n      82.0\n      22.0\n      13.1\n      47.1\n      2.5\n      7.1\n      0.3\n      2.7\n      0.1\n      0.0\n    \n    \n      3\n      4.0\n      6.0\n      2012.0\n      25.0\n      89.0\n      13.0\n      2.5\n      28.6\n      1.3\n      6.9\n      0.0\n      1.7\n      0.0\n      0.0\n    \n    \n      4\n      5.0\n      6.0\n      2012.0\n      27.0\n      77.0\n      16.0\n      0.0\n      64.8\n      3.0\n      14.2\n      1.2\n      3.9\n      0.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      242\n      26.0\n      9.0\n      2012.0\n      30.0\n      65.0\n      14.0\n      0.0\n      85.4\n      16.0\n      44.5\n      4.5\n      16.9\n      6.5\n      1.0\n    \n    \n      243\n      27.0\n      9.0\n      2012.0\n      28.0\n      87.0\n      15.0\n      4.4\n      41.1\n      6.5\n      8.0\n      0.1\n      6.2\n      0.0\n      0.0\n    \n    \n      244\n      28.0\n      9.0\n      2012.0\n      27.0\n      87.0\n      29.0\n      0.5\n      45.9\n      3.5\n      7.9\n      0.4\n      3.4\n      0.2\n      0.0\n    \n    \n      245\n      29.0\n      9.0\n      2012.0\n      24.0\n      54.0\n      18.0\n      0.1\n      79.7\n      4.3\n      15.2\n      1.7\n      5.1\n      0.7\n      0.0\n    \n    \n      246\n      30.0\n      9.0\n      2012.0\n      24.0\n      64.0\n      15.0\n      0.2\n      67.3\n      3.8\n      16.5\n      1.2\n      4.8\n      0.5\n      0.0\n    \n  \n\n243 rows × 14 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are a total of 13 features: day, month, year, temperature in Celsius, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial speed index, build up index, and fire weather index. Our target label is “fire”, classifying wether or not a forest fire occured given the conditions."
  },
  {
    "objectID": "wildfire_risk_main_code.html#the-portugal-dataset",
    "href": "wildfire_risk_main_code.html#the-portugal-dataset",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "The Portugal Dataset:",
    "text": "The Portugal Dataset:\nThe data collected in this dataset was taken from Montesino park in the northeast region of Portugal. This dataset contains many of the same weather characteristics as the Algerian dataset, but contains only fire events. Rather than using this dataset to predict whether a fire occurred in a given area, it can be used to predict the size of a fire. We will perform the same cleanup for the Portugal dataset using the Pandas function get_dummies().\n\nportugal_df = pd.get_dummies(portugal_df)\n\nportugal_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X\n      Y\n      FFMC\n      DMC\n      DC\n      ISI\n      temp\n      RH\n      wind\n      rain\n      ...\n      month_nov\n      month_oct\n      month_sep\n      day_fri\n      day_mon\n      day_sat\n      day_sun\n      day_thu\n      day_tue\n      day_wed\n    \n  \n  \n    \n      0\n      7\n      5\n      86.2\n      26.2\n      94.3\n      5.1\n      8.2\n      51\n      6.7\n      0.0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      7\n      4\n      90.6\n      35.4\n      669.1\n      6.7\n      18.0\n      33\n      0.9\n      0.0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      7\n      4\n      90.6\n      43.7\n      686.9\n      6.7\n      14.6\n      33\n      1.3\n      0.0\n      ...\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      8\n      6\n      91.7\n      33.3\n      77.5\n      9.0\n      8.3\n      97\n      4.0\n      0.2\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      8\n      6\n      89.3\n      51.3\n      102.2\n      9.6\n      11.4\n      99\n      1.8\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      512\n      4\n      3\n      81.6\n      56.7\n      665.6\n      1.9\n      27.8\n      32\n      2.7\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      513\n      2\n      4\n      81.6\n      56.7\n      665.6\n      1.9\n      21.9\n      71\n      5.8\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      514\n      7\n      4\n      81.6\n      56.7\n      665.6\n      1.9\n      21.2\n      70\n      6.7\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      515\n      1\n      4\n      94.4\n      146.0\n      614.7\n      11.3\n      25.6\n      42\n      4.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      516\n      6\n      3\n      79.5\n      3.0\n      106.7\n      1.1\n      11.8\n      31\n      4.5\n      0.0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n517 rows × 30 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are 12 features: x-axis spatial coordinate, y-axis spatial coordinate, month, day, fine fuel moisture code, duff moisture code, drought code, initial speed index, temperature in Celsius, relative humidity, wind, and rain. Our target label is “fire area”, which determines the total burned area of the fire given the conditions."
  },
  {
    "objectID": "wildfire_risk_main_code.html#training-data-for-algeria",
    "href": "wildfire_risk_main_code.html#training-data-for-algeria",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Training data for Algeria:",
    "text": "Training data for Algeria:\n\nalg_train, alg_test = train_test_split(algeria_df, test_size=0.2, random_state=25)\n\nalg_train\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      day\n      month\n      year\n      Temperature\n      RH\n      Ws\n      Rain\n      FFMC\n      DMC\n      DC\n      ISI\n      BUI\n      FWI\n      Fire\n    \n  \n  \n    \n      105\n      14.0\n      9.0\n      2012.0\n      22.0\n      76.0\n      26.0\n      8.3\n      47.4\n      1.1\n      7.0\n      0.4\n      1.6\n      0.1\n      0.0\n    \n    \n      96\n      5.0\n      9.0\n      2012.0\n      29.0\n      75.0\n      16.0\n      0.0\n      80.8\n      3.4\n      24.0\n      2.8\n      5.1\n      1.7\n      1.0\n    \n    \n      91\n      31.0\n      8.0\n      2012.0\n      28.0\n      80.0\n      21.0\n      16.8\n      52.5\n      8.7\n      8.7\n      0.6\n      8.3\n      0.3\n      0.0\n    \n    \n      78\n      18.0\n      8.0\n      2012.0\n      36.0\n      54.0\n      18.0\n      0.0\n      89.4\n      20.0\n      110.9\n      9.7\n      27.5\n      16.1\n      1.0\n    \n    \n      32\n      3.0\n      7.0\n      2012.0\n      32.0\n      76.0\n      20.0\n      0.7\n      63.1\n      2.6\n      9.2\n      1.3\n      3.0\n      0.5\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      61\n      1.0\n      8.0\n      2012.0\n      36.0\n      45.0\n      14.0\n      0.0\n      78.8\n      4.8\n      10.2\n      2.0\n      4.7\n      0.9\n      0.0\n    \n    \n      146\n      22.0\n      6.0\n      2012.0\n      33.0\n      46.0\n      14.0\n      1.1\n      78.3\n      8.1\n      8.3\n      1.9\n      7.7\n      1.2\n      0.0\n    \n    \n      222\n      6.0\n      9.0\n      2012.0\n      34.0\n      71.0\n      14.0\n      6.5\n      64.5\n      3.3\n      9.1\n      1.0\n      3.5\n      0.4\n      0.0\n    \n    \n      62\n      2.0\n      8.0\n      2012.0\n      35.0\n      55.0\n      12.0\n      0.4\n      78.0\n      5.8\n      10.0\n      1.7\n      5.5\n      0.8\n      0.0\n    \n    \n      135\n      11.0\n      6.0\n      2012.0\n      31.0\n      42.0\n      21.0\n      0.0\n      90.6\n      18.2\n      30.5\n      13.4\n      18.0\n      16.7\n      1.0\n    \n  \n\n194 rows × 14 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ny_train_algeria = alg_train[\"Fire\"]\ny_test_algeria = alg_test[\"Fire\"]\n\nX_train_algeria = alg_train.drop([\"Fire\"], axis=1)\nX_test_algeria = alg_test.drop([\"Fire\"], axis=1)"
  },
  {
    "objectID": "wildfire_risk_main_code.html#training-data-for-portugal",
    "href": "wildfire_risk_main_code.html#training-data-for-portugal",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Training data for Portugal:",
    "text": "Training data for Portugal:\n\nportugal_train, portugal_test = train_test_split(portugal_df, test_size=0.2, random_state=25)\nportugal_df[portugal_df[\"area\"]==0.00]\n\nportugal_train\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X\n      Y\n      FFMC\n      DMC\n      DC\n      ISI\n      temp\n      RH\n      wind\n      rain\n      ...\n      month_nov\n      month_oct\n      month_sep\n      day_fri\n      day_mon\n      day_sat\n      day_sun\n      day_thu\n      day_tue\n      day_wed\n    \n  \n  \n    \n      250\n      8\n      5\n      93.1\n      157.3\n      666.7\n      13.5\n      26.8\n      25\n      3.1\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      48\n      4\n      4\n      87.2\n      23.9\n      64.7\n      4.1\n      11.8\n      35\n      1.8\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n    \n      417\n      6\n      5\n      93.4\n      17.3\n      28.3\n      9.9\n      13.8\n      24\n      5.8\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      140\n      2\n      5\n      90.9\n      126.5\n      686.5\n      7.0\n      21.9\n      39\n      1.8\n      0.0\n      ...\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n    \n      96\n      3\n      4\n      83.9\n      8.0\n      30.2\n      2.6\n      12.7\n      48\n      1.8\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      317\n      4\n      4\n      92.4\n      96.2\n      739.4\n      8.6\n      19.2\n      24\n      4.9\n      0.0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      143\n      1\n      2\n      90.0\n      51.3\n      296.3\n      8.7\n      16.6\n      53\n      5.4\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      474\n      4\n      3\n      93.0\n      103.8\n      316.7\n      10.8\n      26.4\n      35\n      2.7\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      318\n      6\n      5\n      92.8\n      119.0\n      783.5\n      7.5\n      21.6\n      27\n      2.2\n      0.0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      132\n      4\n      6\n      89.3\n      51.3\n      102.2\n      9.6\n      10.6\n      46\n      4.9\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n413 rows × 30 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ny_train_portugal = portugal_train[\"area\"]\ny_test_portugal = portugal_test[\"area\"]\n\nX_train_portugal = portugal_train.drop([\"area\"], axis=1)\nX_test_portugal = portugal_test.drop([\"area\"], axis=1)"
  },
  {
    "objectID": "wildfire_risk_main_code.html#portugals-important-features",
    "href": "wildfire_risk_main_code.html#portugals-important-features",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Portugal’s important features:",
    "text": "Portugal’s important features:\nAfter applying RidgeCV to the Portugal dataset, we yield the 5 most important features: X, Y, ISI, temp, and wind. We will then plot these features against the target label, “Fire Area”, to see their relationship to one another. In each scatterplot, the x-axis represents the important feature and the y-axis shows the amount of burned area given that feature.\n\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X_train_portugal, y_train_portugal) \n\n#assign importance to each feature through a specific attribute\nimportance = np.abs(ridge.coef_)           \n\n#create a plot to visualize the importance of the features\nfeature_names = np.array(X_train_portugal.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature importances via coefficients Portugal\")\nplt.show()\n\n#select a certain number of features\nthreshold = np.sort(importance)[-10]\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X_train_portugal, y_train_portugal)\n\n#only select the first five features from the list\nselected_features_portugal = feature_names[sfm.get_support()][0:5]\n\nprint(f\"Features selected: {selected_features_portugal}\")\n\n\n\n\nFeatures selected: ['X' 'Y' 'ISI' 'temp' 'wind']\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (10, 5))\nfig.subplots_adjust(wspace = 0.5, hspace = 0.4)\nfig.delaxes(ax = axarr[1,2])\n\nfor i in range(5):\n  ax = axarr.flatten()\n  ax[i].scatter(X_train_portugal[selected_features_portugal[i]], y_train_portugal)\n  ax[i].set(ylabel = \"Fire Area\", xlabel = selected_features_portugal[i], title = (f\"Fire area in Portugal in relation to {selected_features_portugal[i]}\"))"
  },
  {
    "objectID": "wildfire_risk_main_code.html#algerias-important-features",
    "href": "wildfire_risk_main_code.html#algerias-important-features",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Algeria’s important features:",
    "text": "Algeria’s important features:\nWe apply the same technique to the Algeria dataset, and the 5 most important features are: RH, Rain, FFMC, ISI, and FWI. We will also plot these features against the target label, “Fire”, to see their relationship to one another. Since this target label is binary, “no fire” or “fire, we opted for box plots to have better visualization.\n\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X_train_algeria, y_train_algeria) \n\n#assign importance to each feature through a specific attribute\nimportance = np.abs(ridge.coef_)\n\n#create a plot to visualize the importance of the features\nfeature_names = np.array(X_train_algeria.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature importances via coefficients in Algeria\")\nplt.show()\n\n#select a certain number of features\nthreshold = np.sort(importance)[-5]\n\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X_train_algeria, y_train_algeria)\n#only select the first three features from the list\nselected_features_algeria = feature_names[sfm.get_support()][0:5]\n\nprint(f\"Features selected by SelectFromModel: {selected_features_algeria}\")\n\n\n\n\nFeatures selected by SelectFromModel: [' RH' 'Rain ' 'FFMC' 'ISI' 'FWI']\n\n\n\nimport seaborn as sns\n\nfig, axes = plt.subplots(2,3, figsize=(18, 10))\nfig.subplots_adjust(wspace = 0.5, hspace = 0.3)\n\nsns.boxplot(ax=axes[0,0], x = y_train_algeria == 1,y=X_train_algeria[selected_features_algeria[0]])\nsns.boxplot(ax=axes[0,1], x = y_train_algeria == 1,y=X_train_algeria[selected_features_algeria[1]])\nsns.boxplot(ax=axes[1,0], x = y_train_algeria == 1,y=X_train_algeria[selected_features_algeria[2]])\nsns.boxplot(ax=axes[1,1], x = y_train_algeria == 1,y=X_train_algeria[selected_features_algeria[3]])\nsns.boxplot(ax=axes[1,2], x = y_train_algeria == 1,y=X_train_algeria[selected_features_algeria[4]])\n\nfig.delaxes(ax=axes[0,2])\n\naxes[0,0].set(title = selected_features_algeria[0])\naxes[0,1].set(title = selected_features_algeria[1])\naxes[1,0].set(title = selected_features_algeria[2])\naxes[1,1].set(title = selected_features_algeria[3])\naxes[1,2].set(title = selected_features_algeria[4])\n\n[Text(0.5, 1.0, 'FWI')]"
  },
  {
    "objectID": "wildfire_risk_main_code.html#training-on-algeria-dataset",
    "href": "wildfire_risk_main_code.html#training-on-algeria-dataset",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Training on Algeria dataset:",
    "text": "Training on Algeria dataset:\nWe will train the algeria data on 4 different classifiers: 1. Decision Tree 2. Logistic Regression 3. Random Forest 4. Stochastic Gradient Descent\nAgain, we will train the Algeria with its completed set of features and with its 5 slected features. The 5 selected features are: RH, Rain, FFMC, ISI, and FWI. In addition, we will use cross validation to evaluate each model’s performance at different settings and avoid overfitting.\n\nDecision tree classifier on Algeria data\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nmax_depth = 5           #iterator for selected features\nmax_depthC = 5          #iterator for complete features\n\nbest_score_DT = 0       #keeping track of highest accuracy for selected features\nbest_score_DTC = 0      #keeping track of highest accuracy for complete features\n\nprint(\"Training on selected features.\")\n\n#train on decision tree classifier using selected features\nwhile max_depth != 0:\n    DT_algeria = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=max_depth))\n    DT_algeria.fit(X_train_algeria[selected_features_algeria], y_train_algeria)\n\n    cv_scores = cross_val_score(DT_algeria, X_train_algeria[selected_features_algeria], y_train_algeria, cv=5)\n    mean_score = cv_scores.mean()\n    print(f\"Max depth = {max_depth}, score = {mean_score.round(3)}\")\n\n    #keeping the list of columns for the max_depth with the best score \n    if (DT_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria) > best_score_DT):\n        best_score_DT = DT_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria)\n        best_DT = DT_algeria\n        best_max_depth = max_depth\n    \n    max_depth += -1\n\nprint(\"\\n\\n\")\nprint(\"Training on all features.\")\n\n#train on decision tree classifier using all features\nwhile max_depthC != 0:\n    DT_algeria_complete = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=max_depthC))\n    DT_algeria_complete.fit(X_train_algeria, y_train_algeria)\n    \n    cv_scores = cross_val_score(DT_algeria_complete, X_train_algeria, y_train_algeria, cv=5)\n    mean_score = cv_scores.mean()\n    print(f\"Max depth = {max_depthC}, score = {mean_score.round(3)}\")\n\n    #keeping the list of columns for the max_depth with the best score \n    if (DT_algeria_complete.score(X_train_algeria, y_train_algeria) > best_score_DTC):\n        best_score_DTC = DT_algeria_complete.score(X_train_algeria, y_train_algeria)\n        best_DTC = DT_algeria_complete\n        best_max_depth = max_depthC\n    \n    max_depthC += -1\n\nTraining on selected features.\nMax depth = 5, score = 0.974\nMax depth = 4, score = 0.974\nMax depth = 3, score = 0.979\nMax depth = 2, score = 0.979\nMax depth = 1, score = 0.974\n\n\n\nTraining on all features.\nMax depth = 5, score = 0.974\nMax depth = 4, score = 0.969\nMax depth = 3, score = 0.979\nMax depth = 2, score = 0.979\nMax depth = 1, score = 0.969\n\n\n\n\nLogistic regression classifier on Algeria\n\nfrom sklearn.linear_model import LogisticRegression\n\n#train on selected features via logistic regression\nLR_algeria = LogisticRegression()\nLR_algeria.fit(X_train_algeria[selected_features_algeria], y_train_algeria)\n\nprint(f\"Training Accuracy for LR using selected features: {LR_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria)}\")\n\n#train on complete dataset\nLR_algeria_complete = LogisticRegression()\nLR_algeria_complete.fit(X_train_algeria, y_train_algeria)\n\nprint(f\"\\nTraining Accuracy for LR using all features: {LR_algeria_complete.score(X_train_algeria, y_train_algeria)}\\n\")\n\nTraining Accuracy for LR using selected features: 0.9742268041237113\n\nTraining Accuracy for LR using all features: 0.9896907216494846\n\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\nRandom forest classifier on Algeria\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmax_depth = 5           #iterator for selected features\nmax_depthC = 5          #iterator for complete features\n\nbest_score_RF = 0       #keeping track of highest accuracy for selected features\nbest_score_RFC = 0      #keeping track of highest accuracy for complete features\n\nprint(\"Training on selected features.\")\n\n#train on random forest classifier using selected features\nwhile max_depth != 0:\n    RF_algeria = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=max_depth, random_state=0))\n    RF_algeria.fit(X_train_algeria[selected_features_algeria], y_train_algeria)\n\n    cv_scores_RF_algeria = cross_val_score(RF_algeria, X_train_algeria[selected_features_algeria], y_train_algeria, cv=5)\n    mean_score = cv_scores_RF_algeria.mean()\n    print(f\"Max depth = {max_depth}, score = {mean_score.round(3)}\")\n\n    #keeping the list of columns for the max_depth with the best score \n    if (RF_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria) > best_score_RF):\n        best_score_RF = RF_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria)\n        best_RF = RF_algeria\n        best_max_depth = max_depth\n    \n    max_depth += -1\n  \nprint(\"\\n\\n\")\nprint(\"Training on all features.\")\n\n#train on random forest classifier using all features\nwhile max_depthC != 0:\n    RF_algeria_complete = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=max_depthC, random_state=0))\n    RF_algeria_complete.fit(X_train_algeria, y_train_algeria)\n    \n    cv_scores_RF_algeria_complete = cross_val_score(RF_algeria_complete, X_train_algeria, y_train_algeria, cv=5)\n    mean_score = cv_scores_RF_algeria_complete.mean()\n    print(f\"Max depth = {max_depthC}, score = {mean_score.round(3)}\")\n\n    # keeping the list of columns for the max_depth with the best score \n    if (RF_algeria_complete.score(X_train_algeria, y_train_algeria) > best_score_RFC):\n        best_score_RFC = RF_algeria_complete.score(X_train_algeria, y_train_algeria)\n        best_RFC = RF_algeria_complete\n        best_max_depth = max_depthC\n    \n    max_depthC += -1\n\nTraining on selected features.\nMax depth = 5, score = 0.974\nMax depth = 4, score = 0.974\nMax depth = 3, score = 0.974\nMax depth = 2, score = 0.974\nMax depth = 1, score = 0.974\n\n\n\nTraining on all features.\nMax depth = 5, score = 0.974\nMax depth = 4, score = 0.974\nMax depth = 3, score = 0.974\nMax depth = 2, score = 0.974\nMax depth = 1, score = 0.943\n\n\n\n\nStochastic gradient descent on Algeria\n\nfrom sklearn.linear_model import SGDClassifier\n\nSGD_algeria = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10)\nSGD_algeria.fit(X_train_algeria[selected_features_algeria], y_train_algeria)\n\nprint(f\"Training Accuracy for SGD using selected features: {SGD_algeria.score(X_train_algeria[selected_features_algeria], y_train_algeria)}\")\n\nSGD_algeria_complete = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10)\nSGD_algeria_complete.fit(X_train_algeria, y_train_algeria)\n\nprint(f\"\\nTraining Accuracy for SGD using all features: {SGD_algeria_complete.score(X_train_algeria, y_train_algeria)} \\n\")\n\n\nTraining Accuracy for SGD using selected features: 0.8556701030927835\n\nTraining Accuracy for SGD using all features: 0.5567010309278351 \n\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn("
  },
  {
    "objectID": "wildfire_risk_main_code.html#training-on-portugal-dataset",
    "href": "wildfire_risk_main_code.html#training-on-portugal-dataset",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Training on Portugal dataset:",
    "text": "Training on Portugal dataset:\nWe will train the algeria data on 4 different classifiers: 1. Linear Regression 2. Logistic Regression 3. Support Vector Classification 4. Epsilon-Support Vector Regression\nHere, we will train the models on Portugal’s 5 slected features from RidgeCV, features highlighted by the paper, and it’s complete set of features. The 5 selected features from RidgeCV are: X, Y, ISI, temp, and wind. The features highlighted by the paper are: temp, RH, rain, and wind.\nWe will also perform transformations on our target label to improve the models’ accuracy. We transformed our target labels to log, binary, range, and categorical values.\nIn addition, we will use cross validation to evaluate each model’s performance at different settings and avoid overfitting.\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\n#fetures highlighted by paper\npapers_features = [\"temp\", \"RH\", \"rain\", \"wind\"]\n\n\n#convert labels to categorical values\nlab = preprocessing.LabelEncoder()\ny_train_transformed = lab.fit_transform(y_train_portugal)\n\n\n# make a copy of df and change labels to 0 and non 0s (binary)\nportugal_df_copy = portugal_df.copy()\nportugal_df_copy.loc[portugal_df_copy[\"area\"] > 0.0] = 1\nportugal_df_copy.loc[portugal_df_copy[\"area\"] <= 0.0] = 0\n\nportugal_train_binary, portugal_test_binary = train_test_split(portugal_df_copy, test_size=0.2, random_state=25)\n\ny_train_portugal_binary = portugal_train_binary[\"area\"]\ny_test_portugal_binary = portugal_test_binary[\"area\"]\n\nX_train_portugal_binary = portugal_train_binary.drop([\"area\"], axis=1)\nX_test_portugal_binary = portugal_test_binary.drop([\"area\"], axis=1)\n\n\n\n# make a copy of df and change labels to a range\nportugal_df_copy2 = portugal_df.copy()\n\nprint(portugal_df_copy2[\"area\"].max())\nranges = portugal_df_copy2[\"area\"].max()  / 50\n\nportugal_df_copy2.loc[portugal_df_copy2[\"area\"] <= 0.0] = 0.0\n\nfor i in range(50):\n  portugal_df_copy2.loc[(portugal_df_copy2[\"area\"] > (ranges * i)) & (portugal_df_copy2[\"area\"] <= ranges * (i+1))] = ranges * (i+1)\n\nportugal_train_range, portugal_test_range = train_test_split(portugal_df_copy2, test_size=0.2, random_state=25)\n\ny_train_portugal_range = portugal_train_range[\"area\"]\ny_test_portugal_range = portugal_test_range[\"area\"]\n\nX_train_portugal_range = portugal_train_range.drop([\"area\"], axis=1)\nX_test_portugal_range = portugal_test_range.drop([\"area\"], axis=1)\n\n1090.84\n\n\n\nLinear regression classifier on Portugal\n\nfrom sklearn.linear_model import LinearRegression\n\n#train on non-transformed y\nLinR_portugal = LinearRegression()\nLinR_portugal.fit(X_train_portugal, y_train_portugal)\n\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal.score(X_train_portugal, y_train_portugal)))\n\nTraining Accuracy for linear regression: 0.04793115962477279\n\n\n\n#train on y-train that was split into 50 ranges\nLinR_portugal = LinearRegression()\nLinR_portugal.fit(X_train_portugal_range, y_train_portugal_range)\n\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal.score(X_train_portugal_range, y_train_portugal_range)))\n\nTraining Accuracy for linear regression: 1.0\n\n\n\n#train on y transformed into binary labels\nLinR_portugal_complete_binary = LinearRegression()\nLinR_portugal_complete_binary.fit(X_train_portugal_binary, y_train_portugal_binary)\n\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal_complete_binary.score(X_train_portugal_binary, y_train_portugal_binary)))\n\nTraining Accuracy for linear regression: 1.0\n\n\n\n#train on y transformed into categorical values\n\n#train on paper's features\nLinR_portugal_paper = LinearRegression()\nLinR_portugal_paper.fit(X_train_portugal[papers_features], y_train_transformed)\n\ncv_scores_portugal = cross_val_score(LinR_portugal_paper, X_train_portugal[papers_features], y_train_transformed, cv=5)\n\nprint(f\"Cross val scores for features highlited by paper: {cv_scores_portugal}\")\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal_paper.score(X_train_portugal[papers_features], y_train_transformed)))\n\n\n#train on all features\nLinR_portugal_complete = LinearRegression()\nLinR_portugal_complete.fit(X_train_portugal, y_train_transformed)\n\ncv_scores_portugal = cross_val_score(LinR_portugal_complete, X_train_portugal, y_train_transformed, cv=5)\n\nprint(f\"\\nCross val scores for all features: {cv_scores_portugal}\")\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal_complete.score(X_train_portugal, y_train_transformed)))\n\n\n#train on featured selected by RidgeCV\nLinR_portugal_selected = LinearRegression()\nLinR_portugal_selected.fit(X_train_portugal[selected_features_portugal], y_train_transformed)\n\ncv_scores_portugal = cross_val_score(LinR_portugal_selected, X_train_portugal[selected_features_portugal], y_train_transformed, cv=5)\n\nprint(f\"\\nCross val scores for features selected by RidgeCV: {cv_scores_portugal}\")\nprint(\"Training Accuracy for linear regression: \" + str(LinR_portugal_selected.score(X_train_portugal[selected_features_portugal], y_train_transformed)))\n\nCross val scores for features highlited by paper: [ 0.01875518 -0.03799198 -0.01599223 -0.97997841 -0.01931605]\nTraining Accuracy for linear regression: 0.014415847547068927\n\nCross val scores for all features: [ 0.02493997 -0.06590393 -0.06223167 -1.37012855 -0.21533972]\nTraining Accuracy for linear regression: 0.07364043558950595\n\nCross val scores for features selected by RidgeCV: [ 0.01855795 -0.05131269 -0.02041167  0.00572244 -0.11218235]\nTraining Accuracy for linear regression: 0.021379727090125145"
  },
  {
    "objectID": "wildfire_risk_main_code.html#logistic-regression-on-portugal",
    "href": "wildfire_risk_main_code.html#logistic-regression-on-portugal",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Logistic Regression on Portugal",
    "text": "Logistic Regression on Portugal\n\n#train on paper's features and using binary labels \nLogR_portugal_binary = LogisticRegression()\nLogR_portugal_binary.fit(X_train_portugal_binary, y_train_portugal_binary)\n\nprint(f\"\\nTraining Accuracy for logistic regression: {LogR_portugal_binary.score(X_train_portugal_binary, y_train_portugal_binary)}\")\n\n\nTraining Accuracy for logistic regression: 1.0\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n#train on y transformed into categorical values\n\n#train on paper's features\nLogR_portugal_paper = LogisticRegression()\nLogR_portugal_paper.fit(X_train_portugal[papers_features], y_train_transformed)\n\nprint(f\"\\nTraining Accuracy for logistic regression using features highlighted by the paper: {LogR_portugal_paper.score(X_train_portugal[papers_features], y_train_transformed)}\\n\")\n\n#train on complete dataset\nLogR_portugal_complete = LogisticRegression()\nLogR_portugal_complete.fit(X_train_portugal, y_train_transformed)\n\nprint(f\"\\nTraining Accuracy for logistic regression using all features: {LogR_portugal_complete.score(X_train_portugal, y_train_transformed)}\\n\")\n\n#train on features selected by RidgeCV\nLogR_portugal_selected = LogisticRegression()\nLogR_portugal_selected.fit(X_train_portugal[selected_features_portugal], y_train_transformed)\n\nprint(f\"\\nTraining Accuracy for logistic regression using features selected by RidgeCV: {LogR_portugal_selected.score(X_train_portugal[selected_features_portugal], y_train_transformed)}\\n\")\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nTraining Accuracy for logistic regression using features highlighted by the paper: 0.48184019370460046\n\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nTraining Accuracy for logistic regression using all features: 0.46246973365617433\n\n\nTraining Accuracy for logistic regression using features selected by RidgeCV: 0.513317191283293\n\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "wildfire_risk_main_code.html#support-vector-classifcation-on-portugal",
    "href": "wildfire_risk_main_code.html#support-vector-classifcation-on-portugal",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Support Vector Classifcation on Portugal",
    "text": "Support Vector Classifcation on Portugal\n\nfrom sklearn import svm\n#train on y transformed into categorical values\n\n#train on paper's features\nSVC_portugal_paper = svm.SVC(kernel='poly', degree=3)\nSVC_portugal_paper.fit(X_train_portugal[papers_features], y_train_transformed)\nprint(f\"\\nTraining Accuracy for SVC using features highlighted from the paper: {SVC_portugal_paper.score(X_train_portugal[papers_features], y_train_transformed)}\")\n\n#train on all features\nSVC_portugal_complete = svm.SVC(kernel='poly', degree=3)\nSVC_portugal_complete.fit(X_train_portugal, y_train_transformed)\nprint(f\"\\nTraining Accuracy for SVC using all features: {SVC_portugal_complete.score(X_train_portugal, y_train_transformed)}\")\n\n#train on features selected by RidgeCV\nSVC_portugal_selected = svm.SVC(kernel='poly', degree=3)\nSVC_portugal_selected.fit(X_train_portugal[selected_features_portugal], y_train_transformed)\nprint(f\"\\nTraining Accuracy for SVC using features selected by RidgeCV: {SVC_portugal_selected.score(X_train_portugal[selected_features_portugal], y_train_transformed)}\")\n\n\nTraining Accuracy for SVC using features highlighted from the paper: 0.48184019370460046\n\nTraining Accuracy for SVC using all features: 0.4794188861985472\n\nTraining Accuracy for SVC using features selected by RidgeCV: 0.49878934624697335\n\n\n\nSVR on Portugal\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\n\n#training on y transformed gave worst accuracy, so we went ahead and trained on the original y labels\n\n#train on paper's features\nSVR_portugal_paper = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_paper.fit(X_train_portugal[papers_features], y_train_portugal)\n\ncv_scores_SVR = cross_val_score(SVR_portugal_paper, X_train_portugal[papers_features], y_train_portugal, cv=5)\n\nprint(f\"Cross val scores for selected features: {cv_scores_SVR}\")\nprint(\"Training Accuracy for SVC: \" + str(SVR_portugal_paper.score(X_train_portugal[papers_features], y_train_portugal)))\n\n#train on all features\nSVR_portugal_complete = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_complete.fit(X_train_portugal, y_train_portugal)\n\ncv_scores_SVR = cross_val_score(SVR_portugal_complete, X_train_portugal, y_train_portugal, cv=5)\n\nprint(f\"\\nCross val scores for all features: {cv_scores_SVR}\")\nprint(\"Training Accuracy for SVC: \" + str(SVR_portugal_complete.score(X_train_portugal, y_train_portugal)))\n\n#train on features selected by RidgeCV\nSVR_portugal_selected = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_selected.fit(X_train_portugal[selected_features_portugal], y_train_portugal)\n\ncv_scores_SVR = cross_val_score(SVR_portugal_selected, X_train_portugal[selected_features_portugal], y_train_portugal, cv=5)\n\nprint(f\"\\nCross val scores for features selected by RidgeCV: {cv_scores_SVR}\")\nprint(\"Training Accuracy for SVC: \" + str(SVR_portugal_selected.score(X_train_portugal[selected_features_portugal], y_train_portugal)))\n\nCross val scores for selected features: [-0.02603315 -0.16955323 -0.20086096 -0.17891782 -0.03004511]\nTraining Accuracy for SVC: -0.026964073944544698\n\nCross val scores for all features: [-0.02601154 -0.16531766 -0.20141533 -0.16730645 -0.03065345]\nTraining Accuracy for SVC: -0.0259967252082014\n\nCross val scores for features selected by RidgeCV: [-0.02547019 -0.14594411 -0.18518067 -0.14792468 -0.03294225]\nTraining Accuracy for SVC: -0.02569288926430735\n\n\n\n\nLog-transform then fit again\nWe’ll log-transform the values and then train the models on them again to see if this yields better accuracy.\n\n#log-transform target labels\ny_train_log_transformed = np.log(y_train_portugal+1)\n\n#using linear regression to train on log-transformed y\n\n#paper's features\nLinR_portugal_paper_log = LinearRegression()\nLinR_portugal_paper_log.fit(X_train_portugal[papers_features], y_train_log_transformed)\n\npredictions = LinR_portugal_paper_log.predict(X_train_portugal[papers_features])\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"Training Accuracy for linear regression after log-transform with paper's features: {score}\")\n\n#all features\nLinR_portugal_complete_log = LinearRegression()\nLinR_portugal_complete_log.fit(X_train_portugal, y_train_log_transformed)\n\npredictions = LinR_portugal_complete_log.predict(X_train_portugal)\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"\\nTraining Accuracy for linear regression after log-transform with all features: {score}\")\n\n#RidgeCV features\nLinR_portugal_selected_log = LinearRegression()\nLinR_portugal_selected_log.fit(X_train_portugal[selected_features_portugal], y_train_log_transformed)\n\npredictions = LinR_portugal_selected_log.predict(X_train_portugal[selected_features_portugal])\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"\\nTraining Accuracy for linear regression after log-transform with ridgeCV features: {score}\")\n\nTraining Accuracy for linear regression after log-transform with paper's features: -0.02135912961710429\n\nTraining Accuracy for linear regression after log-transform with all features: -0.01670568262633898\n\nTraining Accuracy for linear regression after log-transform with ridgeCV features: -0.01953918401847443\n\n\n\n#using SVR to train on log-transformed y\n\n#paper's features\nSVR_portugal_paper_log = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_paper_log.fit(X_train_portugal[papers_features], y_train_log_transformed)\n\npredictions = SVR_portugal_paper_log.predict(X_train_portugal[papers_features])\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"Training Accuracy for SVR after log-transform: {score}\")\n\n#all features\nSVR_portugal_complete_log = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_complete_log.fit(X_train_portugal, y_train_log_transformed)\n\npredictions = SVR_portugal_complete_log.predict(X_train_portugal)\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"\\nTraining Accuracy for SVR after log-transform: {score}\")\n\n#ridgeCV features\nSVR_portugal_selected_log = make_pipeline(StandardScaler(), SVR(gamma='auto'))\nSVR_portugal_selected_log.fit(X_train_portugal[selected_features_portugal], y_train_log_transformed)\n\npredictions = SVR_portugal_selected_log.predict(X_train_portugal[selected_features_portugal])\npredictions_transformed = np.exp(predictions) - 1\n\nu = ((y_train_portugal - predictions_transformed)** 2).sum()\nv = ((y_train_portugal - y_train_portugal.mean()) ** 2).sum()\nscore = 1 - u/v\nprint(f\"\\nTraining Accuracy for SVR after log-transform: {score}\")\n\nTraining Accuracy for SVR after log-transform: -0.025953112452074434\n\nTraining Accuracy for SVR after log-transform: -0.02414013462072151\n\nTraining Accuracy for SVR after log-transform: -0.0248659859046918"
  },
  {
    "objectID": "posts/blog6/index.html",
    "href": "posts/blog6/index.html",
    "title": "Dr. Timmit Gebru",
    "section": "",
    "text": "Dr. Timnit Gebru is a renowned computer scientist and a leading voice in the field of artificial intelligence. She was born and raised in Ethiopia and later earned her PhD from Stanford University in Electrical Engineering. Dr. Gebru is best known for her work on bias and fairness in AI, and she has been instrumental in bringing attention to the issue of under-representation of women and minorities in the tech industry. In addition to her research, Dr. Gebru is also a co-founder of Black in AI, an organization dedicated to increasing representation of Black people in AI research and industry. She has been recognized with numerous awards for her contributions to the field, including being named one of the 100 most influential African Americans by The Root magazine.\nOn April 24th, 2023, Dr. Gebru will give a virtual talk at Middlebury College discussing bias social impacts of artificial intelligence."
  },
  {
    "objectID": "posts/blog6/index.html#computer-vision-in-practice-who-is-benefiting-and-who-is-being-harmed",
    "href": "posts/blog6/index.html#computer-vision-in-practice-who-is-benefiting-and-who-is-being-harmed",
    "title": "Dr. Timmit Gebru",
    "section": "“Computer vision in practice: who is benefiting and who is being harmed?”",
    "text": "“Computer vision in practice: who is benefiting and who is being harmed?”\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition 2020. In the talk, Dr. Gebru raised several important points about the need for fairness and ethical considerations in computer vision and machine learning.\nDr. Gebru focused on the issue of bias in different computer vision systems. She highlighted that many computer vision systems are trained on biased and limited datasets, which can lead to discriminatory outcomes. For example, facial recognition systems have been shown to have higher error rates for women and people with darker skin tones, as compared to white males. Thus, Dr. Gebru emphasized the importance of addressing bias in these systems and called for greater diversity in the datasets used to train them.\nAdditionally, she discussed the need for transparency and accountability in the development and deployment of machine learning models. She argued that it is crucial to understand how these models work and to be able to identify when they are making mistakes. Moreover, Dr. Gebru emphasized the importance of allowing individuals to understand and control the data that is being collected about them, especially the communities most affected by these computer vision systems. She argued that these communities should have a say in how the systems are designed and used, and that their perspectives should be taken into account when making decisions about the technology.\nDr. Gebru also called for greater awareness and education around the ethical implications of computer vision technology. She emphasized the need for researchers, developers, and policymakers to work together to ensure that these systems are developed in ways that are fair, transparent, and accountable.\nOverall, her talk was a powerful call to action for the computer vision community to prioritize fairness and ethics in their work."
  },
  {
    "objectID": "posts/blog6/index.html#tldr",
    "href": "posts/blog6/index.html#tldr",
    "title": "Dr. Timmit Gebru",
    "section": "tl;dr",
    "text": "tl;dr\nThere is a strong need for greater diversity, transparency, and community involvement in the development of computer vision systems because these systems can unintentionally create harm to many groups of people."
  },
  {
    "objectID": "posts/blog6/index.html#proposed-question",
    "href": "posts/blog6/index.html#proposed-question",
    "title": "Dr. Timmit Gebru",
    "section": "Proposed Question:",
    "text": "Proposed Question:\n\nHow do you propose ethics and biases in the field of computer science should be taught to better improve the curriculum?"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog1/perceptron.py"
  },
  {
    "objectID": "posts/blog1/index.html#the-perceptron-algorithm",
    "href": "posts/blog1/index.html#the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "The Perceptron Algorithm",
    "text": "The Perceptron Algorithm\nThe algorithm updates its guesses (weights) as it runs and also keeps a record of its history of accuracy. It processes one example at a time. For a given example, it makes a prediction and checks to see if this prediction is correct. If the prediction is correct, it does nothing. Otherwise, it changes its parameters do better on this example next time around. The algorithm stops once it reaches a user-specified maximum number of steps or the accuracy is 100%."
  },
  {
    "objectID": "posts/blog1/index.html#source-code",
    "href": "posts/blog1/index.html#source-code",
    "title": "Perceptron",
    "section": "Source Code",
    "text": "Source Code\nThe algorithm has three main functions: fit(), predict(), and score().\nWe pass into fit() a feature matrix \\(X \\in R^{n \\times p}\\) and a target vector \\(y \\in R^n\\). The function updates the weights and keeps track of the history of its accuracy. Its goal is to find a good \\(\\tilde{w}\\) to separate the data points. Fit() utilizes the following function when updating its guesses:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + y_i \\tilde{x}_i\\;\\]\nScore() and predict() compares the target variable \\(y_i\\) to the predicted label \\(\\hat{y}_i^{(t)}\\). If \\(\\hat{y}^{(t)}_i y_i > 0\\), then we do nothing because the point is correctly classified. Otherwise, we perform the update on \\(\\tilde{w}\\).\n\n\nCode\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w \nprint(\"Evolution of the score over the training period:\") \nprint(p.history[-10:]) #just the last few scores\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Perceptron Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]"
  },
  {
    "objectID": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nAfter the perceptron algorithm is run, the weight vector \\(\\tilde{w}\\) generates a hyperplane that perfectly separates data points from both classes on either side of it.\n\n\nCode\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Linearly Seperable Data with Perceptron\")\nplt.savefig(\"image.jpg\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X, y)\n\n\n1.0"
  },
  {
    "objectID": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Nonconvergence of the Perceptron Algorithm",
    "text": "Nonconvergence of the Perceptron Algorithm\nWhen the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{w}\\) , but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\n\nCode\nfig = plt.scatter(X_n[:,0], X_n[:,1], c = y_n)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim(-1,3)\ntitle = plt.title(\"Nonseperable Data with Perceptron\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X_n, y_n)\n\n\n0.5"
  },
  {
    "objectID": "posts/blog7/index.html",
    "href": "posts/blog7/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Extra parameters added to svd_reconstruct\nWe will change a few things to our function to allow user to be able to specific the compression ratio and the epsilon threshold.\n\n\nCode\ndef user_svd_reconstruct(img, comp_factor, threshold):\n    U, sigma, V = np.linalg.svd(img)\n\n    M = img.shape[0]\n    N = img.shape[1]\n\n    # k must be less than m and n\n    k = round((N * M) / ((N + M + 1) * comp_factor))\n\n    # create the D matrix in the SVD\n    D = np.zeros_like(img,dtype=float)                       # matrix of zeros of same shape as A\n    for value in sigma:\n        if (value > threshold): \n            D[:min(img.shape),:min(img.shape)] = np.diag(sigma)      # singular values on the main diagonal\n    \n    U = U[:,:k]\n    D = D[:k, :k]\n    V = V[:k, :]\n\n    img = U @ D @ V \n\n    return  img\n\n\nThe user_svd_reconstruct function have three arguments: the image to reconstruct, and the compression factor, and a desired threshold epsilon. The user can enter the desired compression factor and threshold epsilon. We will go ahead and experiment on different compression ratios and thresholds.\n\n\nCode\nreconstructed_img = user_svd_reconstruct(grey_img, 0.2, 1.5)\ncompare_images(grey_img, reconstructed_img)"
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog2/LogisticRegression.py"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Regular Gradient Descent",
    "text": "Accuracy of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period (last few scores):\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period (last few scores):\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Regular Gradient Descent",
    "text": "Empirical Risk of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period (last few losses):\") \nprint(LR.loss_history[-10:]) #just the last few losses\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Evolution of the Loss Function\")\n\n\nEvolution of the loss over the training period (last few losses):\n[0.12756465548793788, 0.12755823771168123, 0.12755183457409008, 0.1275454460315202, 0.12753907204049475, 0.12753271255770313, 0.12752636754000055, 0.127520036944407, 0.12751372072810663, 0.12751372072810663]"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Stochastic Gradient Descent",
    "text": "Accuracy of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period:\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Stochastic Gradient Descent",
    "text": "Empirical Risk of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period:\") \nprint(LR.loss_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Loss Function for Stochastic Gradient\")\n\n\nEvolution of the loss over the training period:\n[0.12482870714265093, 0.1248063720546357, 0.12478314821058926, 0.1247593290029095, 0.12473696450872014, 0.12471412006229605, 0.1246933964041914, 0.12467358666704516, 0.12465223491810985, 0.12465223491810985]"
  },
  {
    "objectID": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Choice of Batch Size in Stochastic Gradient Descent",
    "text": "Choice of Batch Size in Stochastic Gradient Descent\nBelow is an illustration in which the choice of batch size influences how quickly the algorithm converges.\n\n\nCode\np_features = 11\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.1, 100, 50)\n\nfig, axarr = plt.subplots(1, 2)\n\nfig = axarr[0].plot(LR.loss_history)\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\nfig = axarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"image.jpg\")"
  },
  {
    "objectID": "posts/blog5/index.html",
    "href": "posts/blog5/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Our goals for this blog post is to: 1. Train a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race. 2. Perform a bias audit of our algorithm to determine whether it displays racial bias.\nThere are approximately 48,000 rows of PUMS data in this data frame. Each one corresponds to an individual citizen of the state of Alabama who filled out the 2018 edition of the PUMS survey. We will filter through this dataset to predict employment status on the basis of demographics excluding race, and audit for racial bias. We will fit the training data on the Decision Tree Classifier model from scikit-learn and perform cross-validation to select the best max depth to achieve the highest accuracy.\n\n\nCode\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000049\n      6\n      1\n      1600\n      3\n      1\n      1013097\n      75\n      19\n      ...\n      140\n      74\n      73\n      7\n      76\n      75\n      80\n      74\n      7\n      72\n    \n    \n      1\n      P\n      2018GQ0000058\n      6\n      1\n      1900\n      3\n      1\n      1013097\n      75\n      18\n      ...\n      76\n      78\n      7\n      76\n      80\n      78\n      7\n      147\n      150\n      75\n    \n    \n      2\n      P\n      2018GQ0000219\n      6\n      1\n      2000\n      3\n      1\n      1013097\n      118\n      53\n      ...\n      117\n      121\n      123\n      205\n      208\n      218\n      120\n      19\n      123\n      18\n    \n    \n      3\n      P\n      2018GQ0000246\n      6\n      1\n      2400\n      3\n      1\n      1013097\n      43\n      28\n      ...\n      43\n      76\n      79\n      77\n      80\n      44\n      46\n      82\n      81\n      8\n    \n    \n      4\n      P\n      2018GQ0000251\n      6\n      1\n      2701\n      3\n      1\n      1013097\n      16\n      25\n      ...\n      4\n      2\n      29\n      17\n      15\n      28\n      17\n      30\n      15\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\n\nWe’ll focus on a relatively small number of features in the modeling tasks of this blog post. Here are all the possible features we will use:\n\n\nCode\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\n\nCode\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nWe will go ahead and split our testing and training data.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/blog5/index.html#intersectional-trends",
    "href": "posts/blog5/index.html#intersectional-trends",
    "title": "Auditing Allocative Bias",
    "section": "Intersectional trends",
    "text": "Intersectional trends\n\n\nCode\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\ng = sns.catplot(\n    data=df, kind=\"bar\",\n    x=\"group\", y=\"label\", hue = \"SEX\",\n    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n)\ng.despine(left=True)\ng.set_axis_labels(\"Race\", \"Employment\")\ng.savefig(\"image.jpg\")\n\n\n\n\n\nThe chart above displays the intersectional trend between race and sex when checking for those who are employed.Men are displayed using the blue bars, and women are shown with the red bars. Overall, men have a higher employment rate across different races. But we will check to see how many women and men were accounted for in this dataset.\n\n\nCode\ndf.groupby('SEX')[['label']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n\n  \n    \n      \n      label\n    \n    \n      \n      mean\n      len\n    \n    \n      SEX\n      \n      \n    \n  \n  \n    \n      1.0\n      0.45\n      18369\n    \n    \n      2.0\n      0.37\n      19852\n    \n  \n\n\n\n\nWe can see that 18,369 men and 19,852 women were accounted for in the data. Yet, men have a much higher employment rate as compared to women. 45% of men are employed and only 37% of women are employed. These differences were also seen through our intersectional Race x Gender chart above."
  },
  {
    "objectID": "posts/blog5/index.html#overall-measures",
    "href": "posts/blog5/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\n\nCode\ny_hat = best_DT.predict(X_test)\n\nprint(\"The overall accuracy in predicting whether someone is employed is: \")\nprint((y_hat == y_test).mean())\n\n\nThe overall accuracy in predicting whether someone is employed is: \n0.8115320217664295\n\n\n\n\nCode\nmatrix = confusion_matrix(y_test, y_hat)\n\ntp = matrix[1][1]\ntn = matrix[0][0]\nfp = matrix[0][1]\nfn = matrix[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative rate: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive rate: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7652757078986587\n\nFalse negative rate: 0.15479204339963834\n\nFalse positive rate: 0.16817939135077417\n\n\nWe used the confusion matrix function from sklearn to understand the kind of mistakes that the model most frequently makes. The overall accuracy of our model is 81%, with a positive predictive value of 0.77. Additionally, the overall false negative is 15.48% and overall false positive is 16.82%. It’s clear that our model makes mistakes. Beyond that, it seems as though the model makes different kinds of error in its prediction for different groups."
  },
  {
    "objectID": "posts/blog5/index.html#by-group-measures",
    "href": "posts/blog5/index.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe’re going to compare the model’s confusion matrices on the test data for white and black individuals to see if there exists bias in the model’s performance.\n\n\nCode\nprint(\"The accuracy for white individuals is: \")\nprint((y_hat == y_test)[group_test == 1].mean())\n\nprint(\"\\nThe accuracy for black individuals is: \")\nprint((y_hat == y_test)[group_test == 2].mean())\n\n\nThe accuracy for white individuals is: \n0.810126582278481\n\nThe accuracy for black individuals is: \n0.8151589242053789\n\n\nIt seems like the model attains similar accuracy score when predicting white and black individuals. It achieves 81.0% accuracy when predicting employment for white individuals and 81.5% for black individuals.\n\n\nCode\n# white sub group\nmatrix_white = confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1])\n\ntp = matrix_white[1][1]\ntn = matrix_white[0][0]\nfp = matrix_white[0][1]\nfn = matrix_white[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative for white individuals: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive for white individuals: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7782139352306182\n\nFalse negative for white individuals: 0.16580310880829016\n\nFalse positive for white individuals: 0.1670362158167036\n\n\n\n\nCode\n# black sub group\nmatrix_black = confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2])\n\ntp = matrix_black[1][1]\ntn = matrix_black[0][0]\nfp = matrix_black[0][1]\nfn = matrix_black[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative for black individuals: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive for black individuals: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7238805970149254\n\nFalse negative for black individuals: 0.12570507655116842\n\nFalse positive for black individuals: 0.16985462892119357\n\n\nWhen breaking down the matrices, the predictive value is a bit higher for the white individuals, with a difference of 5%. The model has similar false positive predictions for both black and white individuals. The false negative predictions are also comparable, except with a small difference of 4%. The model tends to predict unemployment for white individuals even when they are employed more than it does for the black individuals."
  },
  {
    "objectID": "posts/blog5/index.html#bias-measures",
    "href": "posts/blog5/index.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nI conclude that the model is well calibrated because it reflects the same likelihood of recidivism irrespective of the individuals’ group membership. In other words, it is free from predictive bias with respect to race. Additionally, since the false positive and false negative rates are similar across both group, the model satisfies approximate error rate balance. Lastly, our model achieves statistical parity because the proportion of individuals classified for employment is the same for each group."
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Creating the testing and validation data\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\n\nCode\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\nplt.savefig(\"image.jpg\")\n\n\n\n\n\n\n\nUsing an analytical formula to implement least-squares linear regression\nI utilized the following analytical formula for the optimal weight vector from the lecture notes to implement least-squares linear regression.\n\\[\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;\\]\n\n\nCode\nfrom LinearRegression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nprint(f\"The estimated weight vector is {LR.w}\")\n\n\nTraining score = 0.1794\nValidation score = 0.2142\nThe estimated weight vector is [0.40551937 0.79095312]\n\n\n\n\nImplementing gradient descent for linear regression\nThe formula for the gradient is:\n\\[ \\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;\\]\n\n\nCode\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nprint(f\"The estimated weight vector using gradient descent is {LR2.w}\")\n\n\nThe estimated weight vector using gradient descent is [0.4055465  0.79093819]\n\n\n\n\nCode\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nIncreasing p_features\nBelow, we will perform an experiment in which p_features, the number of features used, will increase, while holding n_train, the number of training points, constant.\n\n\nCode\n#increasing p_features \np_features = n_train - 1\n\nfor p in range(1, p_features):\n    #generate data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p, noise)\n     #fit data to linear regression model\n    LR3 = LinearRegression()\n    LR3.fit_analytic(X_train, y_train)  \n    #score the model \n    score_train = LR3.score(X_train, y_train)\n    score_val = LR3.score(X_val, y_val)\n        \n    if p == 1:\n        plt.scatter(p, score_train, color = \"blue\", label = \"training\")\n        plt.scatter(p, score_val, color = \"red\", label = \"validation\") \n    else:\n        plt.scatter(p, score_train, color = \"blue\")\n        plt.scatter(p, score_val, color = \"red\") \n\n#plot\nlabels = plt.gca().set(xlabel = \"P features\", ylabel = \"Score\")\nlegend = plt.legend(loc='lower right')\n    \n\n\n\n\n\nAs we can see, the training score achieves a high accuracy very quickly as we increases the number of features. However, our validation score fails to achieve a high accuracy score as we increase the number of features. This happened because our model runs into the issue of overfitting. Our model was not able to generalize well, and performed poorly when encountering new data. The model focused too closely on the noise in the training data to the extend that it negatively impacts the performance of the validation data.\n\n\nLASSO Regularization\nBelow, we replicate the same experiment as the one above by increasing the number of features, but using LASSO instead of linear regression.\nThe LASSO algorithm uses a modified loss function with a regularization term:\n\\[L(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;\\]\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\np_features = n_train + 10\n\nfor p in range(1, p_features):\n    #generate data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p, noise)\n    #fit data to LASSO\n    L = Lasso(alpha = 0.01)\n    L.fit(X_train, y_train)  \n    #score the model \n    score_train = L.score(X_train, y_train)\n    score_val = L.score(X_val, y_val)\n        \n    if p == 1:\n        plt.scatter(p, score_train, color = \"blue\", label = \"training\")\n        plt.scatter(p, score_val, color = \"red\", label = \"validation\") \n    else:\n        plt.scatter(p, score_train, color = \"blue\")\n        plt.scatter(p, score_val, color = \"red\") \n\nlabels = plt.gca().set(xlabel = \"P features\", ylabel = \"Score\")\nlegend = plt.legend(loc='lower right')\n\n\n\n\n\nThe LASSO algorithm performs much more poorly compared to linear regression, especially as you increase the value of the regularization strength (alpha). As we can expect, our model performs very well on the training data as the number of features used increases, but not on the validation data. The validation data experiences overfitting, just like our linear regression model, except the LASSO algorithm performs much worse on the validation data."
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Below is the Palmer Penguins data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER. The data contains physiological measurements for a number of penguins from each of three species of penguin: chinstrap, gentoo, and adelie.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\nOur data frame has some columns we can’t use directly. Thus, we used the “one-hot encoding” to represent these features as binary columns.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\n\n\nThe displayed figure below shows the general relationship between sex and body mass (g) in each of the 3 penguin species. By looking at the figure, it’s clear that female penguins tend to have a lower body mass than the male penguins. In addition, the Gentoo penguins have a larger overall body mass than the other 2 species, weighing between 4000 to 6000 grams. Both the Adelie and Chinstap species weigh between 3000 to 4500 grams.\n\n\nCode\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=train, x=\"Body Mass (g)\", y=\"Sex\", hue=\"Species\")\nylab = ax.set(ylabel=\"Sex\")\n\n\n\n\n\n\n\n\nThe table below shows clear differences in flipper lengths between the 3 species. Gentoo penguins have the largest overall flipper length at 217.65mm. The Chinstrap penguins’ overall flipper length comes in at 195.46mm. Adelie penguins have the smallest overall flipper length of the 3 species, at 189.97mm. However, it’s important to note that there was much less data recorded for the Chinstrap species as compared to the other 2 species.\n\n\nCode\ntrain.groupby(['Species'])[['Flipper Length (mm)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n\n  \n    \n      \n      Flipper Length (mm)\n    \n    \n      \n      mean\n      len\n    \n    \n      Species\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      189.97\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      195.46\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      217.65\n      101"
  },
  {
    "objectID": "posts/blog3/index.html#cross-validation",
    "href": "posts/blog3/index.html#cross-validation",
    "title": "Classifying Palmer Penguins",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nWe will use cross-validation to get a sense for what max depth we should set for the Decision Tree Classifier.\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores = cross_val_score(best_DT, X_train[best_cols_DT], y_train, cv=5)\ncv_scores\n\n\narray([1.        , 0.94117647, 0.98039216, 0.98039216, 0.98039216])\n\n\n\n\nCode\n#code provided by Professor Phil; taken from class lecture\nmax_depth = 5\n\nwhile max_depth != 0:\n  DT = DecisionTreeClassifier(max_depth = max_depth)\n  cv_scores = cross_val_score(DT, X_train[best_cols_DT], y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Max depth = {max_depth}, score = {mean_score.round(3)}\")\n  max_depth += -1\n\n\nMax depth = 5, score = 0.98\nMax depth = 4, score = 0.98\nMax depth = 3, score = 0.976\nMax depth = 2, score = 0.945\nMax depth = 1, score = 0.731"
  },
  {
    "objectID": "posts/blog3/index.html#plotting-decision-regions",
    "href": "posts/blog3/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\n\nCode\n#Code provided by Professor Phil; taken from lecture notes.\nfrom matplotlib.patches import Patch\n\nwarnings.filterwarnings(\"ignore\")\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nPlotting decision regions for the Logistic Regression model on its 3 best chosen features: culmen length, culmen depth, and island.\n\n\nCode\nplot_regions(best_LR, X_train[best_cols_LR], y_train)\n\n\n\n\n\nPlotting decision regions for the Random Forest classifier on its 3 best chosen features: culmen length, flipper length, and island.\n\n\nCode\nplot_regions(best_RF, X_train[best_cols_RF], y_train)\n\n\n\n\n\nPlotting decision regions for the Decision Tree classifier on its 3 best chosen features: culmen length, flipper length, and island.\n\n\nCode\nplot_regions(best_DT, X_train[best_cols_DT], y_train)"
  },
  {
    "objectID": "posts/final blog/index.html",
    "href": "posts/final blog/index.html",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "",
    "text": "Git repository for project: https://github.com/ebwieman/wildfire-risk-tool/tree/main\nSource code: https://github.com/ndang01/ndang01.github.io/blob/main/wildfire_risk_main_code.ipynb\nMapping tool: https://github.com/ebwieman/wildfire-risk-tool/blob/main/wildfire_risk_mapping_tool.ipynb"
  },
  {
    "objectID": "posts/final blog/index.html#abstract",
    "href": "posts/final blog/index.html#abstract",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Abstract",
    "text": "Abstract\nWildfires have increased in recent years as a result of climate change, posing a threat to humans and the environment. Understanding the conditions that lead to wildfire and which areas are most at risk is important for developing mitigation strategies and allocating resources. In this project, we utilize machine learning algorithms to predict both wildfire occurrence and area of wildfires. To predict wildfire occurrence, we trained several machine learning models on a dataset containing meteorological information about potential wildfires in Algeria. An accuracy of 100% was achieved using a logistic regression model trained on all features. To predict wildfire area, models were trained on a dataset of wildfire events from Montesinho National Park, Portugal. The area prediction task was much more difficult than the classification task and required additional label transformation to achieve higher accuracies. The highest area prediction accuracy was achieved with a logistic regression model trained on all features and using labels transformed with the scikit-learn lab encoder. Lastly, a model trained on the Algerian dataset was used to predict and map wildfire risk in the United States. The trained model was more simplistic than other models due to the lack of US meteorological data available, but visual comparison to existing fire prediction tools such as the USGS Fire Danger Map Tool shows that the model was somewhat able to predict fire risk in the United States. One shortcoming of this work is that all datasets used to train our models were small and regionally specific, making it difficult to create generalizable tools for use on larger scales or different regions. Developing larger and more regionally dispersed wildfire datasets will aid in future creation of more robust fire prediction tools."
  },
  {
    "objectID": "posts/final blog/index.html#introduction",
    "href": "posts/final blog/index.html#introduction",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Introduction",
    "text": "Introduction\nClimate change has increased the likelihood of a variety of extreme weather events, including wildfires. These extreme events pose definite risks to both human and ecological communities. At the same time, machine learning is emerging as an important predictive tool in natural disaster prevention; numerous studies have already used machine learning to classify risk of natural hazards. For example, Youssef et al. used machine learning models to predict an area’s susceptibility to landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023). This study experimented with a few different models, ultimately settling on Random Forest. In Choubin et al.’s study of avalanches in mountainous regions, Support Vector Machine (SVM) and Multivariate Discriminant Analysis were found to be the best models in assessing avalanche risk based on meteorological and locational data (Choubin et al. 2019).\nPrior studies have also focused specifically on wildfire prediction. A 2023 paper even developed a new model to better predict burned areas in Africa and South America (Li et al. 2023). In addition, the dataset used in our project to predict Portuguese fires was originally the topic of a 2007 paper focusing on training models on readily available meteorological data to predict the area of wildfires (Cortez and Morais 2007). This study also experimented with different predictive features and models, finding SVM and random forest to be the best predictors (Cortez and Morais 2007).\nIn this project, we build on these prior studies, using many similar techniques and models. We use two datasets to predict wildfire likelihood and wildfire area given meteorological data. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire events with 11 attributes that describe weather characteristics at the time of that event. Our second dataset, the Forest Fires Dataset, contains data from Montesinho National Park, Portugal. This dataset contains many of the same weather characteristics as the Algerian dataset, but contains only fire events, and also includes the areas of these events. Rather than using this dataset to predict whether a fire occurred in a given area, we used it to predict the size a fire is likely to reach given certain meteorological conditions.\nOver the course of this project, we followed in the footsteps of prior studies, experimenting with different models to predict risk. We implemented several existing machine learning models such as Random Forest and Logistic Regression, ultimately choosing the models and sets of features that yielded the highest accuracy score. We trained and validated models on the Algerian dataset to predict whether or not a forest fire will occur given certain meteorological conditions and also trained/validated models on the Portugal dataset to predict forest fire area given many of the same features.\nAlthough we trained our models with localized data, we also assessed the extent to which we could apply our models across datasets and geographies. Similarly to the multi-hazard susceptibility map produced by Youssef et al., we created a fire susceptibility map of fire risk using our models and county-level temperature and precipitation data for the entire United States (Youssef et al. 2023). While we cannot assess the accuracy of this map directly, we can compare it to existing US wildfire prediction tools to at least visually assess our accuracy.\nFinally, we build on existing research to assess the ethics of using machine learning models in predicting risk of natural hazards. As Wagenaar et al. caution in their 2020 study of how machine learning will change flood risk and impact assessment, “applicability, bias and ethics must be considered carefully to avoid misuse” of machine learning algorithms (Wagenaar et al. 2020).\nAs extreme weather events become more prevalent with climate change, we must learn how to best predict and manage these events. The methods explored detail our own attempt to do so."
  },
  {
    "objectID": "posts/final blog/index.html#values-statement",
    "href": "posts/final blog/index.html#values-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Values Statement",
    "text": "Values Statement\nBeing able to accurately predict the risks of wildfires offers numerous benefits for various stakeholders. Predictive models like ours have the potential to revolutionize wildfire management and mitigation strategies, leading to better preparedness, timely response, and ultimately, the protection of lives and property. Thus, we wanted to build upon prior studies and further the ability to accurately predict wildfires.\nThe main beneficiaries of this project are the wildfire departments and emergency responders. These agencies can gain valuable insights into the probability and severity of wildfire occurrences in specific areas though our project. These predictions allow for better allocation of resources and more effective mitigation strategies, reducing the overall damage and saving tons of valuable resources. If we can assess areas with high-risk for wildfires ahead of time, they could be better equipped and protected.\nInsurance companies can also leverage our predictive models to assess risks associated with wildfires. By integrating these algorithms into their underwriting processes, insurers can accurately evaluate the potential impact of wildfires on properties, allowing for more precise risk assessment. Moreover, the models can aid in post–fire analysis, enabling insurance companies to provide timely assistance to policyholders living in at-risk areas.\nGovernment agencies responsible for land and forest management are also another one of our potential users. Policymakers can make more informed decisions regarding land use, forest management, and allocation of resources for fire prevention efforts. Ultimately, this can lead to more proactive measures such as improved firebreak construction and targeted vegetation management.\nAnother group that may not directly utilize our predictive wildfire models, but still stands to benefit is the general population. Information that trickles down from fire management authorities can help individuals living in fire-prone areas make informed decisions and take the necessary precautions. People are able to take property protection measures and implement evacuation plans they deem necessary. Overall, the predictions from these models can increase safety measures while minimizing property damage within the predicted at-risk areas.\nWhile the use of machine learning models to predict wildfire risks can be highly beneficial, there are potential downsides with their implementation that should be considered. Our models rely heavily on well documented weather and geographical data. Data collection takes time and money, which could potentially be a barrier to using our models. Remote areas without government or research funding most likely will not be able to produce the data needed to benefit from our models. Moreover, communities lacking internet access, computing devices, or technological literacy are unable to take advantage of our models. This can disproportionately affect rural areas and low-income communities, further exacerbating existing inequalities.\nAdditionally, we recognize that our models are trained and tested on data that is in English. This language barrier can hinder individuals who don’t have the proficiency in the language, limiting their ability to use these predictive models. Additionally, cultural differences and contextual nuances might not be well captured in our models, leading to potential misunderstandings and biases. Thus, we want to be mindful of the potential barriers and hope to address these shortcomings in our future work. Failing to address these disparities could perpetuate social inequalities in wildfire management.\nUpon reflecting these potential harms and benefits, we still believe this project will improve wildfire management and be a crucial resource to communities in fire-prone areas. Additionally, this project furthers our understanding of factors contributing to wildfires. With this information, we can accurately predict the likelihood of wildfires in a given region, enabling better fire management, mitigation, and evacuation. Wildfire predictions can help protect the land, wildlife, and local communities. Still, efforts should be made to actively involve marginalized groups in wildfire preparedness and response initiatives."
  },
  {
    "objectID": "posts/final blog/index.html#materials-and-methods",
    "href": "posts/final blog/index.html#materials-and-methods",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nThe Datasets\nIn this project, we utilized various machine learning models from scikit-learn to better understand wildfire occurrences. Specifically, we trained and tested on two datasets to predict wildfire likelihood and wildfire area given the meteorological and geographical features. These datasets were taken from the University of California Irvine Machine Learning Repository. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire occurrences with 11 attributes describing weather characteristics at the time the event occurred. Our second dataset, the Forest Fires Data, has data from Montesinho National Park, Portugal. This dataset contains many of the same weather attributes as the Algerian one. However, rather than using this dataset to predict whether a fire occurred in a given area, it can be used to predict the size of the fire given certain meteorological and geographical conditions.\nThe Algerian dataset has 224 instances that regrouped data from two regions in Algeria, namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region in the northwest of Algeria. There are 122 instances for each of the two regions. This data was collected from June 2012 to September 2012. As stated, the dataset contains fire and non-fire events with 13 attributes that describe weather conditions. The 13 features are: day, month, year, temperature in Celsius, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial speed index, build up index, and fire weather index. Our target label is “fire”, with a label of 1.0 meaning that a fire has occurred and 0.0 being no fire.\nThe data collected in the Portuguese dataset was taken from Montesino park in the northeast region of Portugal. The 12 features in this dataset are similar to the ones in the Algerian dataset. These features include: x-axis spatial coordinate, y-axis spatial coordinate, month, day, fine fuel moisture code, duff moisture code, drought code, initial speed index, temperature in Celsius, relative humidity, wind, and rain. The target label is “fire area”, which determines the total burned area of a fire given the meteorological and geographical conditions.\nWe used RidgeCV from scikit-learn to pull out five important features in each dataset that we later trained on. RidgeCV is a cross validation method in ridge regression. The higher absolute coefficient, the more important that feature is. After applying RidgeCV to the Portuguese dataset, we yielded the five most important features: x-axis spatial coordinate, y-axis spatial coordinate, initial speed index, temperature, and wind. The five most important features for the Algerian dataset are: relative humidity, rain, fine fuel moisture code, initial speed index, and fire weather index.\n\n\nModeling\nBoth datasets were trained on various algorithms taken from scikit-learn and each model was analyzed based on their accuracy score.\nThe Algerian training data was trained on four different algorithms: decision tree, logistic regression, random forest, and stochastic gradient descent. For each of the algorithms, we trained the Algerian training set with its complete set of 13 features and also with the 5 features selected via our feature selection process. Additionally, we used cross validation to evaluate the models’ performance at different settings and to avoid overfitting. All five algorithms performed relatively well on the training data, with the accuracy score ranging from 0.552 to 0.990. Training the complete set of features on the stochastic gradient descent classifier yielded the lowest score of 0.552. Alternatively, training on the logistic regression classifier with the complete set of features gave us the highest accuracy of 0.990. Therefore, we went ahead and used the logistic regression model to test our Algerian data on.\nAs for the Portuguese dataset, we started by training it on the linear regression model and support vector machine for regression, but got very low accuracy scores of 0.048 and -0.026, respectively. It was much harder to train on this dataset because the labels were non-binary and many were skewed towards 0’s. We then performed a log-transform on our labels, hoping to improve the accuracy. However, this did not make much of a difference. The highest score we got after log-transforming our labels and training on a linear regression model is -0.020. After this, we went ahead and transformed our labels through three different methods. First, we used the lab encoder in the scikit-learn preprocessing package to encode the target labels with values between 0 and n number of classes - 1. Second, we divided the labels into 50 different ranges. Last, we transformed the labels into binary labels, making the areas larger than 0.0 as “non 0’s” and keeping labels of 0.0 as “0.0”. While we recognize that this method is not ideal, it was very difficult training the Portuguese dataset since it is a very small dataset and most of the target labels are skewed towards 0.0.\nIn the end, we trained our dataset on four different models: linear regression, logistic regression, support vector machine for classification, and support vector machine for regression. Each model was trained on different combinations of features and transformed target labels. We trained the model on its complete set of features, on the features selected via our feature selection process, and on the features highlighted in the research paper written by (Cortez and Morais 2007). In their paper, they also experienced difficulty training this Portuguese dataset. Ultimately, transforming our labels via the lab encoder got us a training score of 0.513 when trained on a logistic regression model using the complete set of features. Unsurprisingly, changing our y into binary labels and into ranges got us a perfect training accuracy of 1.0. Thus, we decided to test our Portuguese data on the logistic regression model with the transformed target labels.\n\n\nUS Meteorological Data and Mapping Tool\nCounty level precipitation and temperature data was downloaded from the NOAA NClimGrid dataset (Durre 2023). Specific months of interest were selected and data was downloaded for that month as two csvs, one containing precipitation data and one containing temperature data. The precipitation data contained total precipitation in mm for each day of the month for each US county, while the temperature data contained maximum temperature in Celsius for each day. Significant time was spent searching for additional meteorological data, specifically wind speed and humidity, but these efforts were ultimately unsuccessful. The temperature and precipitation datasets were uploaded to GitHub and then loaded into the Mapping Tool Jupyter notebook. Substantial data cleaning was performed to prepare the US data for prediction. Data cleaning tasks included renaming columns after the datasets were read in and resolving discrepancies in the state and county codes so that data frames could be merged by county. The precipitation and temperature datasets were read in, converted to pandas dataframes, and merged. From this dataframe, data was extracted for a specified day and county-level wildfire predictions were generated for that day. Predictions were generated using a new Random Forest model that was trained on the Algeria dataset using only Temperature and Rain as features, as these were the only features we had access to for US data. These predictions were then joined with a dataframe containing geological coordinates for each county and then the predictions were mapped. The final algorithm used for mapping allows the user to put in a date and state and then produces a map of county-level fire risk for that state on the specified day. If there is no data for the specified day, the algorithm returns a message apologizing for the lack of data. If no state is specified, the algorithm returns a map of the entire continental US. While the maps could not be rigorously assessed for accuracy, visual comparison to the USGS Fire Danger Map Tool was used to discuss the accuracy of our Algerian model when applied to US data (Eidenshink 2023)."
  },
  {
    "objectID": "posts/final blog/index.html#results",
    "href": "posts/final blog/index.html#results",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Results",
    "text": "Results\nWe came close to achieving all of our goals for this project. Our greatest success was our modeling of the Algerian dataset. We achieved high training accuracy with several models, including > 95% training accuracy for a Random Forest Classifier, Logistic Regression, and Decision Tree Classifier, as well as 100% testing accuracy with a Logistic Regression model. It’s difficult to say how applicable this model is outside of the region of Algeria the data are from. The random forest classifier that we trained on the Algerian model for our mapping tool got many things right – it predicts more widespread fire in the summer months, for example, than in the winter – but we trained it on many fewer features than are in the full Algerian dataset.\nThis is certainly one of the shortcomings of our project. The lack of weather data available for the United States means that we can’t fully say how well our model would perform on data from a region other than Algeria. However, there are positive trends, and were more weather observations available, it seems likely that our model would have some applicability.\nDespite the fact that we did not get to realize the full abilities of our model, creating an interactive map is nonetheless a highlight of the project. It shows that our project has the ability to be broadly applicable, and underscores the power of Machine Learning as a tool for natural hazard prediction and prevention. We explored the ethical dimensions of this application of Machine Learning in a short essay, and while there are certainly ethical considerations that must be made when training models to predict natural hazards, this project shows that there is also a great deal of opportunity to use Machine Learning to predict and manage risks from natural hazards.\nWhile we cannot assess the accuracy of our US prediction tool directly, visual comparison to existing mapping tools such as the USGS Fire Danger Map Tool yields some insights. If we compare the two tools’ predictions for March 2, 2023, we see that our tool vastly overpredicts fire risk in the US (Figure 1). Our tool correctly predicts fire in the locations that the USGS identifies the most high risk, specifically Southwestern Texas and Arizona, but we also predict risk of fire in large swaths of the Midwest and Florida that the USGS tool does not deem particularly high risk.\n \n\nFig 1: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for March 2, 2023.\n\nNext we look at a potentially more interesting case in July 2020, the peak of summer, when we would expect more areas of the country to be susceptible to wildfires. Here the USGS predicts risk of fire in much of the Southwestern United States, and we see that our model does as well. However, we once again see overprediction by our model, specifically in the Midwest. One possible contributing factor is that the USGS model differentiates between agricultural land and does not predict fire risk in these areas. Our model seems to predict fire risk in many of the areas deemed agricultural by the USGS model, so differentiating between land cover classes could be useful for future development of this tool. Additionally, our model predicts on the county scale, while the USGS mapping tool appears to have much better resolution, allowing it to make more refined predictions. One other exciting observation in the July maps is that while our model tends to make generally uniform predictions across states, we do see some agreement between the two tools in identifying pockets of low fire risk within the Southwestern United States. The fact that our mapping tool tends to make relatively uniform predictions across states suggests that the model could just be learning general weather patterns rather than actually learning fire risk. Training on additional features would likely help address this problem, but this is impossible at the moment due to the lack of aggregated US weather data available.\n \n\nFig 2: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for July 2, 2020.\n\nThe final area of our project to discuss is the models we trained for the Portuguese dataset. Replicating the process of the original academic paper didn’t yield results of the same accuracy as those obtained by the study (Cortez and Morais 2007). Log-transforming the data was a complex process that was perhaps more involved than we had anticipated. However, we still managed to achieve about 50% accuracy with both Logistic Regression and Support Vector Machine models, and 100% accuracy when we transformed to binary labels and categorical labels. Figure 3, shown below, shows the results the model achieved on 20 features from the test set:\n\n\n\nimage\n\n\n\nFig 3: While the model correctly predicts 35% of fire areas, it gets close on a number of them, so 35% is not reflective of the actual performance. We see that the model does the best at predicting small fires, though for medium-sized fires, it usually either far overshoots or far undershoots in its prediction.\n\nFigure 4 shows the same observations, predicted using the SVC model. It also gets 35% correct, but unlike the LR model, it only ever underestimates the area of fires, while the LR model overestimates several. This suggests that the SVC model is slightly more conservative, and may fail to predict the largest fires.\n\n\n\nimage\n\n\n\nFig 4: The SVC model seems to be more conservative, predicting the area of all fires – even those that are larger in size – to be 0 ha or close to it.\n\nThese mixed results for the Portuguese data, along with our high testing accuracy for the Algeria dataset, shows that predicting whether or not a fire occurred is a much more straightforward task than predicting the area of the fire. There is certainly room to grow in our modeling of this dataset; perhaps trying different models or features would yield different results. We recognize that transforming the target labels into categorical and binary labels isn’t ideal; however, it was too difficult otherwise. This was because the Portuguese dataset is a very small dataset with the majority of the target labels skewed towards 0.0. Thus, this problem was turned into a classifying problem instead of producing an accurate predictor model. Nonetheless, our process and results show that it is possible to build a model trained on readily-available weather observations that predicts the area of fires with a reasonable degree of accuracy."
  },
  {
    "objectID": "posts/final blog/index.html#concluding-discussion",
    "href": "posts/final blog/index.html#concluding-discussion",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOverall, our project was successful in a number of ways, and opens the door for future research and experimentation. Our analysis of wildfire risk provided us with an opportunity to practice data science. We successfully found regional data sets, cleaned them, and prepared them for machine learning analyses. Using a Ridge model, we successfully selected features to use for making predictions in each dataset, and through experimenting with a variety of different machine learning models, we selected two that reached reasonably high accuracy. Through examining United States county-level data, we were able to apply our models across scales. Finally, we visualized our results well, and effectively discussed their ethical implications.\nAs a group, we met and even surpassed our original project goals. We have a code repository that details our data preparation and cleaning, model training and prediction, and evaluation, notebooks with in exploratory data analysis and testing of our models, a short essay about the implications of using machine learning and automated tools for ecological forecasting (both risks and benefits) and a map of wildfire risk in the United States constructed using our models and US meteorological data. We had wanted to use machine learning to help with natural hazard risk detection and prevention, and our final product provides detailed and thorough documentation of our attempt to do so.\nBecause our project built on the work of scholars such as Cortez and Morias, we have the ability to compare our results to past work. While Cortez and Morias’ 2007 study that explored the Portugal data we used in our project identified SVM to be the best machine learning model for fire area prediction, we found SVC and Logistic Regression to be equally effective on the Portugal data set (Cortez and Morais 2007). Our work weaves in well with prior studies, allowing us to make predictions about natural risk by choosing and applying machine learning models. Overall, through experimenting with different models, we ultimately were able to predict wildfires and wildfire area more accurately than our base rates, and experimented with applying these models across scales. With more time, data, or computational resources, we could improve our methods and findings. Spending more time finding and downloading more data for the United States (such as wind speed and humidity, prehaps) would likely improve our map and our model and allow us to better apply the model across space. While we trained our machine learning models on regionally-specific data, we applied thier predictions to new and differently-scaled geographies. When applying models across space, it is important to do so in collaboration those people who may be affected by the results of predictive machine learning tools (Wagenaar et al. 2020). Overall, when we pay attention to bias and applicabiltiy, wildfire and natural disaster models have numerous applications, and even the potential to save lives. As the likelihood and intensity of extreme weather events continues increasing in the face of climate change, we should incorporate and continue building on the anaysis presented here to acheive even better methodologies to predict and prepare for these events."
  },
  {
    "objectID": "posts/final blog/index.html#group-contributions-statement",
    "href": "posts/final blog/index.html#group-contributions-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nMadeleine: Near the beginning of the project, Wright and I worked on preparing the data from the Algeria and Portugal data sets to eventually split into testing and training data. I then worked on selecting features important for fire/fire area prediction using RidgeCV. We then worked on visualizing the relationships between these selected features and fire likelihood/area. Later in the project, I worked with Eliza on our map. Specifically, I worked on rendering the map to show all the counties in the US, and and helped prepare to join the counties on the map to the county-level temperature and precipitation data. I also worked on the map’s design. Finally, I tried to find additional weather data for the US (such as wind speed), but was largely unsuccessful.\nWright: I worked with Madeleine at the beginning of the project doing some data preparation. Specifically, I worked on formatting the Algeria data set so that it could be treated as binary fire/no fire data. I then worked on data vizualizations, making graphs of the relationships between different features and fire occurrence/scale. I wasn’t directly involved with the modeling, but after the models were trained I ran them on some test observations and vizualized their performance. Finally, I worked on a short research essay about the ethics of using Machine Learning for natural hazard prediction, prevention, and adaptation.\nEliza: I helped with initial downloading and importing of the Portugal and Algeria datasets and splitting them into train and test sets. I initially worked on training models on the Portugal dataset (with little success) and did some research to figure out how the authors of the paper that the dataset originated from achieved higher accuracies. I did some of the initial experimentation with log-transforming our y values, but then I pivoted to working on the mapping tool and Nhi took this part of the project over. I definitely spent most of my time and effort working on the mapping tool for this project with Madeleine. I did extensive research on US county-level meteorological data and identified and downloaded the nClimGrid dataset that we ended up using for our US predictions. I did a lot of the data cleaning of these datasets prior to generating our maps, which Madeleine took the lead on, and I helped streamline the map-making process after creating our first map by converting a lot of our code into reusable functions. For the blog post, I took lead on the Materials and Methods section with Nhi, specifically focusing on the mapping tool methods, and the Abstract.\nNhi: Towards the beginning of the project, I helped with cleaning up our datasets and making sure they are ready to be trained on. For the majority of the project, I did a lot of the training and testing for both of the Portuguese and Algerian datasets. The Algerian dataset was a lot easier to train on, and I was able to obtain a very good accuracy score early on. I trained the Algeria dataset on different models and with different combinations of features to see which would yield the highest accuracy. I did a lot of cross validation and fine tuning to make sure the models weren’t overfitting. Then I tested the highest scoring model on the Algerian testing dataset. Eliza started with training the Portuguese dataset, but did not get very far since this was a much harder dataset to train on. Thus, I took over the modeling for this as well. I started by training the dataset on other models since Eliza had only trained on two models to start. I wasn’t very successful with the other models either, so I explored different ways to transform the target labels since the majority of the labels were skewed towards 0’s. I did a lot of digging around on scikit-learn and while not ideal, I transformed the labels via lab encoder, into binary labels, and into different ranges of the target labels. I then trained different combinations of the features and transformed target labels on different models. I analyzed these models to see how they performed against the testing dataset. Overall, I dissected a lot of different variations of the features and target labels. Additionally, I also combed through our code and cleaned them up. I also added a lot of the descriptors before and after each code section to explain our processes. For the blog post, I wrote the Values Statement, the dataset and modeling in the Materials and Methods section, and added a bit to the Results."
  },
  {
    "objectID": "posts/final blog/index.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "href": "posts/final blog/index.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention",
    "text": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention\nMachine Learning is emerging as an important predictive tool in natural disaster prevention. Numerous studies have used machine learning to classify risk of natural hazards, from landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023), to avalanche risk in mountainous regions (Choubin et al. 2019), to area of forest fires in the tropics (Li et al. 2023). In fact, the Portuguese fire data used in this project was originally the topic of a 2007 paper focusing on training models on readily available real-time meteorological data to predict the area of wildfires (Cortez and Morais 2007).\nSo, there is a growing library of academic literature and projects using Machine Learning models to predict either the occurrence or the scale of natural hazards. Our project builds on these works, using many similar techniques and models. As our high testing accuracy for the Algerian dataset shows, machine learning clearly can be a powerful tool in natural hazard prediction.\nBut this numerical, scientific, approach comes with risks. As Cortez and Morais note in their study of Monteshino National Park in Portugal, most fires are caused by humans, which is why features like day of the week are important. Natural hazards only become natural disasters when they have a negative impact on people’s lives. As Youseff et al. observe in their study of multi-hazard prediction in Saudi Arabia, multiple hazards often occur at the same time, and the impact of natural hazards disproportionately affects impoverished and underdeveloped countries (Youssef et al. 2023). A risk of our approach is that our models focus only on the fires, disregarding both human influence on the landscape that may lead to increased risk of hazards (e.g. overdrawing from local water sources, deforestation, etc.), as well as the impact fires may have on humans. Predicting whether or not a fire will occur, or how large it will be, is only useful if it is applied to help those at risk from fires.\nWagenaar et al.’s 2020 paper “Invited perspectives: How machine learning will change flood risk and impact assessment” touches on some of these ethical considerations (Wagenaar et al. 2020). One risk they bring up is that improved knowledge of flood risk from ML models might result in protection of only high-value land or property owned by the wealthy. That example is certainly transferable to our fire hazard project. They also raise the question of privacy, noting that some people might not want it widely known that their home is considered “at-risk” for flooding, or other hazards. Finally, there is the question of data. The causes of hazards in one place may not cause the same hazard in another, so it is important to understand which human and geographic features influence hazard risk at a local scale rather than trying to train a one-size-fits-all model.\nWith all that in mind, our project needs to come with an asterisk. We have trained models for forest fire occurrence and scale in two specific places. It is therefore unreasonable to expect that our model will perfectly transfer to other places in the world. If our project were to be used for any application beyond the academic, we would need to make sure that its impact – whether that be for risk management, insurance policies, or some other application – be equitable and nondiscriminatory. We are just scratching the surface of using Machine Learning for natural hazard prevention, and while our results show it to be a powerful tool, we must also stay vigilant to make sure that it is a force for good."
  },
  {
    "objectID": "posts/final blog/index.html#personal-reflection",
    "href": "posts/final blog/index.html#personal-reflection",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThe process of creating and implementing a group project driven by personal curiosity on a topic of our choice has been extremely rewarding. I gained knowledge of what it is like to carry out a machine learning project from start to finish. Additionally, I gained more experience coding as a team. Our group worked very well together and made sure to we were on track to meet our goals. We created minimum viable products throughout our projected timeline as opposed to only having something working at the very end.\nIt’s been a long and rewarding process from combing through repositories looking for datasets to making maps off of the models we created. Some parts have been very frustrating, such as converting the datasets to ones we can train and test on and transforming the target labels of the Portuguese dataset hoping to yield better accuracy. Getting extremely low accuracy scores in the negative percentage on the Portuguese training data was both surprising and defeating. However, this has taught me a lot about the process of training on a difficult dataset with skewed target labels. I was able to explore different types of label transformations along with different combinations of features, while taking into account how the models could be overfitting.\nAdditionally, I was deliberate in analyzing the ethical implications of our project and in considering the possible biases that might arise from our processes. Writing about the ethical implications of our project in the project proposal and in the final blog post allowed me to critically think about the various stakeholders and potential impacts of what we were producing. I believe these are important questions to consider when carrying out any new designs and implementations. Our group actively thought about the potential biases and impacts of this project and did our best create an ethical project while making notes of where we might’ve fallen short on to improve these areas in our future work.\nOverall, I am very proud of what we were able to accomplish in this last month of the semester. We have met all of the planned deliverables by producing models for wildfire predictive risks, maps constructed via our models using US meterological data, and an extended analysis for the implications of using machine learning and automated tools for ecological forecasting. While I recognize that we have fallen short in producing an accurate predictive tool using the Portuguese data, we still learned a lot through the process. I will take the skills and tools I’ve learn through this project and through the class to my next stage of life. Knowing the processes and tools in machine learning will be crucial for the software jobs I am seeking. Beyond that, knowing what works well and what doesn’t when working in team is extremely for any job and for life in general. Life is collaborative and any job environment I join will require collaborative skills. Additionally, I gained many important ethical perspectives that I hope to actively incorporate in my future work in this field of computer science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A write up of my final project, which uses machine learning models trained on meteorological data from Algeria and Portugal and applies them to assess wildfire risk in the US.\n\n\n\n\n\n\nMay 18, 2023\n\n\nNhi Dang (in collaboration with Wright Frost, Madeleine Gallop, and Eliza Wieman)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, we will implement and experiment with two machine learning techniques for image compression and image segmentation.\n\n\n\n\n\n\nMay 3, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post is a reflection on Dr. Gebru’s work and her visit at Middlebury College.\n\n\n\n\n\n\nApr 16, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post fits a classifier using data from folktables and perform a bias audit for the algorithm.\n\n\n\n\n\n\nMar 31, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post that uses different machine learning models to determine the smallest number of measurements necessary to determine the species of a penguin.\n\n\n\n\n\n\nMar 15, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post that uses different machine learning models to determine the smallest number of measurements necessary to determine the species of a penguin.\n\n\n\n\n\n\nMar 10, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on optimization algorithms that are based on the gradients of functions.\n\n\n\n\n\n\nMar 2, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm. The algorithm aims to find a rule for separating distinct groups in some data.\n\n\n\n\n\n\nFeb 24, 2023\n\n\nNhi Dang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About CS 451",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "notebook/warmup.html",
    "href": "notebook/warmup.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Pclass\n      Age\n      Siblings/Spouses Aboard\n      Parents/Children Aboard\n      Fare\n      Sex_male\n    \n  \n  \n    \n      0\n      2\n      29.0\n      0\n      0\n      10.500\n      0\n    \n    \n      1\n      3\n      4.0\n      3\n      2\n      27.900\n      1\n    \n    \n      2\n      3\n      6.0\n      4\n      2\n      31.275\n      0\n    \n    \n      3\n      3\n      33.0\n      1\n      1\n      20.525\n      1\n    \n    \n      4\n      3\n      22.0\n      0\n      0\n      9.000\n      1\n    \n  \n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7968970380818053\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = int(1e3)))])\n\n\nplr = poly_LR(3)\nplr.fit(X_train, y_train)\nplr.score(X_train, y_train)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.7842031029619182\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\nplr.score(X_test, y_test).round(4)\n\n0.8258\n\n\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = plr.predict(X_test)\n\nconfusion_matrix(y_test, y_pred)\n\narray([[99, 15],\n       [16, 48]])\n\n\n\ndef compute(y_pred, y_test):\n    count_fp = 0\n    count_fn = 0\n    count_tp = 0\n    count_tn = 0\n\n    for i in range(len(y_pred)):\n        if y_pred[i] == 1 & y_test[i] == 0:\n            count_fp += 1\n        if y_pred[i] == 0 & y_test[i] == 1:\n            count_fn += 1\n        if y_pred[i] == 1 & y_test[i] == 1:\n            count_tp += 1\n        if y_pred[i] == 0 & y_test[i] == 0:\n            count_tn += 1\n        \n    fpr = count_fp / (count_fp + count_tn)\n    fnr = count_fn / (count_fn + count_tp) \n    ppv = count_tp / (count_tp + count_fp)\n    p = (count_fn + count_tp) / (count_tn + count_fp + count_fn + count_tp)\n\n    fpr_2 = (p / (1 - p)) * ((1 - ppv) / ppv) * (1 - fnr) \n    t = (fpr, fpr_2)\n    return t\n\ncompute(y_pred, y_test)\n\n(0.46261682242990654, 0.46261682242990665)\n\n\n\n#Linear regression warm up\nimport numpy as np\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\ndef predict(X, w):\n    return X@w\n\ndef score(X,y,w):\n    y_bar = y.mean()\n    y_hat = predict(X,w)\n\n    top = ((y_hat - y) ** 2 ).sum()\n    bottom = ((y_bar - y) ** 2 ).sum()\n    c = 1 - (top / bottom)\n\n    return c\n\nscore(X, y, w)\n\n0.04948727788767249\n\n\n\n#warm up 4/3\n\n# Note: this requires the ``pillow`` package to be installed\nfrom sklearn.datasets import load_sample_image\nchina = load_sample_image(\"china.jpg\")\nax = plt.axes(xticks=[], yticks=[])\nax.imshow(china);\n\n\n\n\n\ndata = china / 255.0 # use 0...1 scale\ndata = data.reshape(427 * 640, 3)\ndata.shape\n\n(273280, 3)\n\n\n\ndef plot_pixels(data, title, colors=None, N=10000):\n    if colors is None:\n        colors = data\n    \n    # choose a random subset\n    rng = np.random.RandomState(0)\n    i = rng.permutation(data.shape[0])[:N]\n    colors = colors[i]\n    R, G, B = data[i].T\n    \n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    ax[0].scatter(R, G, color=colors, marker='.')\n    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n\n    ax[1].scatter(R, B, color=colors, marker='.')\n    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n\n    fig.suptitle(title, size=20);\n\n\nplot_pixels(data, title='Input color space: 16 million possible colors')\n\n\n\n\n\nimport warnings; warnings.simplefilter('ignore')  # Fix NumPy issues.\n\nfrom sklearn.cluster import MiniBatchKMeans\nkmeans = MiniBatchKMeans(16)\nkmeans.fit(data)\nnew_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n\nplot_pixels(data, colors=new_colors,\n            title=\"Reduced color space: 16 colors\")\n\n\n\n\n\nchina_recolored = new_colors.reshape(china.shape)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6),\n                       subplot_kw=dict(xticks=[], yticks=[]))\nfig.subplots_adjust(wspace=0.05)\nax[0].imshow(china)\nax[0].set_title('Original Image', size=16)\nax[1].imshow(china_recolored)\nax[1].set_title('16-color Image', size=16);\n\n\n\n\n\n#warm up 4/5\n\nimport torch\nimport numpy as np\n\n#tensors created directly from data \ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\n#tensors created from numpy array\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.1976, 0.6343, 0.3324],\n        [0.9060, 0.5094, 0.0333]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n\n\n#warm up 4/12\n\nfrom PIL import Image\nimport urllib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nplt.gca().axis(\"off\")\n\n(-0.5, 639.5, 412.5, -0.5)\n\n\n\n\n\n\nimport numpy as np\n\ndef convolve2d(image, kernel):\n    xKern = kernel.shape[0]\n    yKern = kernel.shape[1]\n    xImg = image.shape[0]\n    yImg = image.shape[1]\n    \n    xOutput = int(((xImg - xKern + 2)) + 1)\n    yOutput = int(((yImg - yKern + 2)) + 1)\n    output = np.zeros(xOutput, yOutput)\n\n    imagePadded = image\n    \n    for y in range(image.shape[1]):\n        # Exit Convolution\n        if y > image.shape[1] - yKern:\n            break\n\n        if y % 1 == 0:\n            for x in range(image.shape[0]):\n                # Go to next row once kernel is out of bounds\n                if x > image.shape[0] - xKern:\n                    break\n                try:\n                    \n                    if x % 1 == 0:\n                        output[x, y] = (kernel * imagePadded[x: x + xKern, y: y + yKern]).sum()\n                except:\n                    break\n\n    return output\n\n\nkernel = np.array([[-1, -1, -1], \n                   [-1,  8, -1], \n                   [-1, -1, -1]])\n\nconvd = convolve2d(img, kernel)\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 8)\nplt.gca().axis(\"off\")\n\nTypeError: Cannot interpret '640' as a data type"
  },
  {
    "objectID": "notebook/classification.html",
    "href": "notebook/classification.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n\ndef training_decision_regions(model, cols, **kwargs):\n    m = model(**kwargs)\n    m.fit(np.array(X_train[cols]), y_train)\n    plot_decision_regions(np.array(X_train[cols]), y_train, clf = m)\n    ax = plt.gca()\n    ax.set(xlabel = cols[0], \n                  ylabel = cols[1], \n                  title = f\"Training accuracy = {m.score(np.array(X_train[cols]), y_train).round(2)}\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, \n              species, \n              framealpha=0.3, \n              scatterpoints=1)\n\n\ntraining_decision_regions(LogisticRegression, cols)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "notebook/end-of-course.html",
    "href": "notebook/end-of-course.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Nhi Dang"
  }
]