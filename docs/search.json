[
  {
    "objectID": "posts/blog6/index.html",
    "href": "posts/blog6/index.html",
    "title": "Dr. Timmit Gebru",
    "section": "",
    "text": "Dr. Timnit Gebru is a renowned computer scientist and a leading voice in the field of artificial intelligence. She was born and raised in Ethiopia and later earned her PhD from Stanford University in Electrical Engineering. Dr. Gebru is best known for her work on bias and fairness in AI, and she has been instrumental in bringing attention to the issue of under-representation of women and minorities in the tech industry. In addition to her research, Dr. Gebru is also a co-founder of Black in AI, an organization dedicated to increasing representation of Black people in AI research and industry. She has been recognized with numerous awards for her contributions to the field, including being named one of the 100 most influential African Americans by The Root magazine.\nOn April 24th, 2023, Dr. Gebru will give a virtual talk at Middlebury College discussing bias social impacts of artificial intelligence."
  },
  {
    "objectID": "posts/blog6/index.html#computer-vision-in-practice-who-is-benefiting-and-who-is-being-harmed",
    "href": "posts/blog6/index.html#computer-vision-in-practice-who-is-benefiting-and-who-is-being-harmed",
    "title": "Dr. Timmit Gebru",
    "section": "“Computer vision in practice: who is benefiting and who is being harmed?”",
    "text": "“Computer vision in practice: who is benefiting and who is being harmed?”\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition 2020. In the talk, Dr. Gebru raised several important points about the need for fairness and ethical considerations in computer vision and machine learning.\nDr. Gebru focused on the issue of bias in different computer vision systems. She highlighted that many computer vision systems are trained on biased and limited datasets, which can lead to discriminatory outcomes. For example, facial recognition systems have been shown to have higher error rates for women and people with darker skin tones, as compared to white males. Thus, Dr. Gebru emphasized the importance of addressing bias in these systems and called for greater diversity in the datasets used to train them.\nAdditionally, she discussed the need for transparency and accountability in the development and deployment of machine learning models. She argued that it is crucial to understand how these models work and to be able to identify when they are making mistakes. Moreover, Dr. Gebru emphasized the importance of allowing individuals to understand and control the data that is being collected about them, especially the communities most affected by these computer vision systems. She argued that these communities should have a say in how the systems are designed and used, and that their perspectives should be taken into account when making decisions about the technology.\nDr. Gebru also called for greater awareness and education around the ethical implications of computer vision technology. She emphasized the need for researchers, developers, and policymakers to work together to ensure that these systems are developed in ways that are fair, transparent, and accountable.\nOverall, her talk was a powerful call to action for the computer vision community to prioritize fairness and ethics in their work."
  },
  {
    "objectID": "posts/blog6/index.html#tldr",
    "href": "posts/blog6/index.html#tldr",
    "title": "Dr. Timmit Gebru",
    "section": "tl;dr",
    "text": "tl;dr\nThere is a strong need for greater diversity, transparency, and community involvement in the development of computer vision systems because these systems can unintentionally create harm to many groups of people."
  },
  {
    "objectID": "posts/blog6/index.html#proposed-question",
    "href": "posts/blog6/index.html#proposed-question",
    "title": "Dr. Timmit Gebru",
    "section": "Proposed Question:",
    "text": "Proposed Question:\n\nHow do you propose ethics and biases in the field of computer science should be taught to better improve the curriculum?"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog1/perceptron.py"
  },
  {
    "objectID": "posts/blog1/index.html#the-perceptron-algorithm",
    "href": "posts/blog1/index.html#the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "The Perceptron Algorithm",
    "text": "The Perceptron Algorithm\nThe algorithm updates its guesses (weights) as it runs and also keeps a record of its history of accuracy. It processes one example at a time. For a given example, it makes a prediction and checks to see if this prediction is correct. If the prediction is correct, it does nothing. Otherwise, it changes its parameters do better on this example next time around. The algorithm stops once it reaches a user-specified maximum number of steps or the accuracy is 100%."
  },
  {
    "objectID": "posts/blog1/index.html#source-code",
    "href": "posts/blog1/index.html#source-code",
    "title": "Perceptron",
    "section": "Source Code",
    "text": "Source Code\nThe algorithm has three main functions: fit(), predict(), and score().\nWe pass into fit() a feature matrix \\(X \\in R^{n \\times p}\\) and a target vector \\(y \\in R^n\\). The function updates the weights and keeps track of the history of its accuracy. Its goal is to find a good \\(\\tilde{w}\\) to separate the data points. Fit() utilizes the following function when updating its guesses:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + y_i \\tilde{x}_i\\;\\]\nScore() and predict() compares the target variable \\(y_i\\) to the predicted label \\(\\hat{y}_i^{(t)}\\). If \\(\\hat{y}^{(t)}_i y_i > 0\\), then we do nothing because the point is correctly classified. Otherwise, we perform the update on \\(\\tilde{w}\\).\n\n\nCode\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w \nprint(\"Evolution of the score over the training period:\") \nprint(p.history[-10:]) #just the last few scores\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Perceptron Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]"
  },
  {
    "objectID": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nAfter the perceptron algorithm is run, the weight vector \\(\\tilde{w}\\) generates a hyperplane that perfectly separates data points from both classes on either side of it.\n\n\nCode\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Linearly Seperable Data with Perceptron\")\nplt.savefig(\"image.jpg\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X, y)\n\n\n1.0"
  },
  {
    "objectID": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Nonconvergence of the Perceptron Algorithm",
    "text": "Nonconvergence of the Perceptron Algorithm\nWhen the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{w}\\) , but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\n\nCode\nfig = plt.scatter(X_n[:,0], X_n[:,1], c = y_n)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim(-1,3)\ntitle = plt.title(\"Nonseperable Data with Perceptron\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X_n, y_n)\n\n\n0.5"
  },
  {
    "objectID": "posts/blog7/index.html",
    "href": "posts/blog7/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Extra parameters added to svd_reconstruct\nWe will change a few things to our function to allow user to be able to specific the compression ratio and the epsilon threshold.\n\n\nCode\ndef user_svd_reconstruct(img, comp_factor, threshold):\n    U, sigma, V = np.linalg.svd(img)\n\n    M = img.shape[0]\n    N = img.shape[1]\n\n    # k must be less than m and n\n    k = round((N * M) / ((N + M + 1) * comp_factor))\n\n    # create the D matrix in the SVD\n    D = np.zeros_like(img,dtype=float)                       # matrix of zeros of same shape as A\n    for value in sigma:\n        if (value > threshold): \n            D[:min(img.shape),:min(img.shape)] = np.diag(sigma)      # singular values on the main diagonal\n    \n    U = U[:,:k]\n    D = D[:k, :k]\n    V = V[:k, :]\n\n    img = U @ D @ V \n\n    return  img\n\n\nThe user_svd_reconstruct function have three arguments: the image to reconstruct, and the compression factor, and a desired threshold epsilon. The user can enter the desired compression factor and threshold epsilon. We will go ahead and experiment on different compression ratios and thresholds.\n\n\nCode\nreconstructed_img = user_svd_reconstruct(grey_img, 0.2, 1.5)\ncompare_images(grey_img, reconstructed_img)"
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog2/LogisticRegression.py"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Regular Gradient Descent",
    "text": "Accuracy of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period (last few scores):\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period (last few scores):\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Regular Gradient Descent",
    "text": "Empirical Risk of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period (last few losses):\") \nprint(LR.loss_history[-10:]) #just the last few losses\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Evolution of the Loss Function\")\n\n\nEvolution of the loss over the training period (last few losses):\n[0.12756465548793788, 0.12755823771168123, 0.12755183457409008, 0.1275454460315202, 0.12753907204049475, 0.12753271255770313, 0.12752636754000055, 0.127520036944407, 0.12751372072810663, 0.12751372072810663]"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Stochastic Gradient Descent",
    "text": "Accuracy of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period:\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Stochastic Gradient Descent",
    "text": "Empirical Risk of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period:\") \nprint(LR.loss_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Loss Function for Stochastic Gradient\")\n\n\nEvolution of the loss over the training period:\n[0.12482870714265093, 0.1248063720546357, 0.12478314821058926, 0.1247593290029095, 0.12473696450872014, 0.12471412006229605, 0.1246933964041914, 0.12467358666704516, 0.12465223491810985, 0.12465223491810985]"
  },
  {
    "objectID": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Choice of Batch Size in Stochastic Gradient Descent",
    "text": "Choice of Batch Size in Stochastic Gradient Descent\nBelow is an illustration in which the choice of batch size influences how quickly the algorithm converges.\n\n\nCode\np_features = 11\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.1, 100, 50)\n\nfig, axarr = plt.subplots(1, 2)\n\nfig = axarr[0].plot(LR.loss_history)\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\nfig = axarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"image.jpg\")"
  },
  {
    "objectID": "posts/blog5/index.html",
    "href": "posts/blog5/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Our goals for this blog post is to: 1. Train a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race. 2. Perform a bias audit of our algorithm to determine whether it displays racial bias.\nThere are approximately 48,000 rows of PUMS data in this data frame. Each one corresponds to an individual citizen of the state of Alabama who filled out the 2018 edition of the PUMS survey. We will filter through this dataset to predict employment status on the basis of demographics excluding race, and audit for racial bias. We will fit the training data on the Decision Tree Classifier model from scikit-learn and perform cross-validation to select the best max depth to achieve the highest accuracy.\n\n\nCode\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000049\n      6\n      1\n      1600\n      3\n      1\n      1013097\n      75\n      19\n      ...\n      140\n      74\n      73\n      7\n      76\n      75\n      80\n      74\n      7\n      72\n    \n    \n      1\n      P\n      2018GQ0000058\n      6\n      1\n      1900\n      3\n      1\n      1013097\n      75\n      18\n      ...\n      76\n      78\n      7\n      76\n      80\n      78\n      7\n      147\n      150\n      75\n    \n    \n      2\n      P\n      2018GQ0000219\n      6\n      1\n      2000\n      3\n      1\n      1013097\n      118\n      53\n      ...\n      117\n      121\n      123\n      205\n      208\n      218\n      120\n      19\n      123\n      18\n    \n    \n      3\n      P\n      2018GQ0000246\n      6\n      1\n      2400\n      3\n      1\n      1013097\n      43\n      28\n      ...\n      43\n      76\n      79\n      77\n      80\n      44\n      46\n      82\n      81\n      8\n    \n    \n      4\n      P\n      2018GQ0000251\n      6\n      1\n      2701\n      3\n      1\n      1013097\n      16\n      25\n      ...\n      4\n      2\n      29\n      17\n      15\n      28\n      17\n      30\n      15\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\n\nWe’ll focus on a relatively small number of features in the modeling tasks of this blog post. Here are all the possible features we will use:\n\n\nCode\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\n\nCode\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nWe will go ahead and split our testing and training data.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/blog5/index.html#intersectional-trends",
    "href": "posts/blog5/index.html#intersectional-trends",
    "title": "Auditing Allocative Bias",
    "section": "Intersectional trends",
    "text": "Intersectional trends\n\n\nCode\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\ng = sns.catplot(\n    data=df, kind=\"bar\",\n    x=\"group\", y=\"label\", hue = \"SEX\",\n    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n)\ng.despine(left=True)\ng.set_axis_labels(\"Race\", \"Employment\")\ng.savefig(\"image.jpg\")\n\n\n\n\n\nThe chart above displays the intersectional trend between race and sex when checking for those who are employed.Men are displayed using the blue bars, and women are shown with the red bars. Overall, men have a higher employment rate across different races. But we will check to see how many women and men were accounted for in this dataset.\n\n\nCode\ndf.groupby('SEX')[['label']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n\n  \n    \n      \n      label\n    \n    \n      \n      mean\n      len\n    \n    \n      SEX\n      \n      \n    \n  \n  \n    \n      1.0\n      0.45\n      18369\n    \n    \n      2.0\n      0.37\n      19852\n    \n  \n\n\n\n\nWe can see that 18,369 men and 19,852 women were accounted for in the data. Yet, men have a much higher employment rate as compared to women. 45% of men are employed and only 37% of women are employed. These differences were also seen through our intersectional Race x Gender chart above."
  },
  {
    "objectID": "posts/blog5/index.html#overall-measures",
    "href": "posts/blog5/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\n\nCode\ny_hat = best_DT.predict(X_test)\n\nprint(\"The overall accuracy in predicting whether someone is employed is: \")\nprint((y_hat == y_test).mean())\n\n\nThe overall accuracy in predicting whether someone is employed is: \n0.8115320217664295\n\n\n\n\nCode\nmatrix = confusion_matrix(y_test, y_hat)\n\ntp = matrix[1][1]\ntn = matrix[0][0]\nfp = matrix[0][1]\nfn = matrix[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative rate: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive rate: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7652757078986587\n\nFalse negative rate: 0.15479204339963834\n\nFalse positive rate: 0.16817939135077417\n\n\nWe used the confusion matrix function from sklearn to understand the kind of mistakes that the model most frequently makes. The overall accuracy of our model is 81%, with a positive predictive value of 0.77. Additionally, the overall false negative is 15.48% and overall false positive is 16.82%. It’s clear that our model makes mistakes. Beyond that, it seems as though the model makes different kinds of error in its prediction for different groups."
  },
  {
    "objectID": "posts/blog5/index.html#by-group-measures",
    "href": "posts/blog5/index.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe’re going to compare the model’s confusion matrices on the test data for white and black individuals to see if there exists bias in the model’s performance.\n\n\nCode\nprint(\"The accuracy for white individuals is: \")\nprint((y_hat == y_test)[group_test == 1].mean())\n\nprint(\"\\nThe accuracy for black individuals is: \")\nprint((y_hat == y_test)[group_test == 2].mean())\n\n\nThe accuracy for white individuals is: \n0.810126582278481\n\nThe accuracy for black individuals is: \n0.8151589242053789\n\n\nIt seems like the model attains similar accuracy score when predicting white and black individuals. It achieves 81.0% accuracy when predicting employment for white individuals and 81.5% for black individuals.\n\n\nCode\n# white sub group\nmatrix_white = confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1])\n\ntp = matrix_white[1][1]\ntn = matrix_white[0][0]\nfp = matrix_white[0][1]\nfn = matrix_white[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative for white individuals: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive for white individuals: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7782139352306182\n\nFalse negative for white individuals: 0.16580310880829016\n\nFalse positive for white individuals: 0.1670362158167036\n\n\n\n\nCode\n# black sub group\nmatrix_black = confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2])\n\ntp = matrix_black[1][1]\ntn = matrix_black[0][0]\nfp = matrix_black[0][1]\nfn = matrix_black[1][0]\n\nppv = tp / (tp + fp)\nprint(f\"\\nPPV: {ppv}\")\n\nprint(f\"\\nFalse negative for black individuals: {fn/(fn+tn)}\")\nprint(f\"\\nFalse positive for black individuals: {fp/(fp+tn)}\")\n\n\n\nPPV: 0.7238805970149254\n\nFalse negative for black individuals: 0.12570507655116842\n\nFalse positive for black individuals: 0.16985462892119357\n\n\nWhen breaking down the matrices, the predictive value is a bit higher for the white individuals, with a difference of 5%. The model has similar false positive predictions for both black and white individuals. The false negative predictions are also comparable, except with a small difference of 4%. The model tends to predict unemployment for white individuals even when they are employed more than it does for the black individuals."
  },
  {
    "objectID": "posts/blog5/index.html#bias-measures",
    "href": "posts/blog5/index.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nI conclude that the model is well calibrated because it reflects the same likelihood of recidivism irrespective of the individuals’ group membership. In other words, it is free from predictive bias with respect to race. Additionally, since the false positive and false negative rates are similar across both group, the model satisfies approximate error rate balance. Lastly, our model achieves statistical parity because the proportion of individuals classified for employment is the same for each group."
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Creating the testing and validation data\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\n\nCode\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\nplt.savefig(\"image.jpg\")\n\n\n\n\n\n\n\nUsing an analytical formula to implement least-squares linear regression\nI utilized the following analytical formula for the optimal weight vector from the lecture notes to implement least-squares linear regression.\n\\[\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;\\]\n\n\nCode\nfrom LinearRegression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nprint(f\"The estimated weight vector is {LR.w}\")\n\n\nTraining score = 0.1794\nValidation score = 0.2142\nThe estimated weight vector is [0.40551937 0.79095312]\n\n\n\n\nImplementing gradient descent for linear regression\nThe formula for the gradient is:\n\\[ \\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;\\]\n\n\nCode\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nprint(f\"The estimated weight vector using gradient descent is {LR2.w}\")\n\n\nThe estimated weight vector using gradient descent is [0.4055465  0.79093819]\n\n\n\n\nCode\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nIncreasing p_features\nBelow, we will perform an experiment in which p_features, the number of features used, will increase, while holding n_train, the number of training points, constant.\n\n\nCode\n#increasing p_features \np_features = n_train - 1\n\nfor p in range(1, p_features):\n    #generate data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p, noise)\n     #fit data to linear regression model\n    LR3 = LinearRegression()\n    LR3.fit_analytic(X_train, y_train)  \n    #score the model \n    score_train = LR3.score(X_train, y_train)\n    score_val = LR3.score(X_val, y_val)\n        \n    if p == 1:\n        plt.scatter(p, score_train, color = \"blue\", label = \"training\")\n        plt.scatter(p, score_val, color = \"red\", label = \"validation\") \n    else:\n        plt.scatter(p, score_train, color = \"blue\")\n        plt.scatter(p, score_val, color = \"red\") \n\n#plot\nlabels = plt.gca().set(xlabel = \"P features\", ylabel = \"Score\")\nlegend = plt.legend(loc='lower right')\n    \n\n\n\n\n\nAs we can see, the training score achieves a high accuracy very quickly as we increases the number of features. However, our validation score fails to achieve a high accuracy score as we increase the number of features. This happened because our model runs into the issue of overfitting. Our model was not able to generalize well, and performed poorly when encountering new data. The model focused too closely on the noise in the training data to the extend that it negatively impacts the performance of the validation data.\n\n\nLASSO Regularization\nBelow, we replicate the same experiment as the one above by increasing the number of features, but using LASSO instead of linear regression.\nThe LASSO algorithm uses a modified loss function with a regularization term:\n\\[L(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;\\]\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\np_features = n_train + 10\n\nfor p in range(1, p_features):\n    #generate data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p, noise)\n    #fit data to LASSO\n    L = Lasso(alpha = 0.01)\n    L.fit(X_train, y_train)  \n    #score the model \n    score_train = L.score(X_train, y_train)\n    score_val = L.score(X_val, y_val)\n        \n    if p == 1:\n        plt.scatter(p, score_train, color = \"blue\", label = \"training\")\n        plt.scatter(p, score_val, color = \"red\", label = \"validation\") \n    else:\n        plt.scatter(p, score_train, color = \"blue\")\n        plt.scatter(p, score_val, color = \"red\") \n\nlabels = plt.gca().set(xlabel = \"P features\", ylabel = \"Score\")\nlegend = plt.legend(loc='lower right')\n\n\n\n\n\nThe LASSO algorithm performs much more poorly compared to linear regression, especially as you increase the value of the regularization strength (alpha). As we can expect, our model performs very well on the training data as the number of features used increases, but not on the validation data. The validation data experiences overfitting, just like our linear regression model, except the LASSO algorithm performs much worse on the validation data."
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Below is the Palmer Penguins data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER. The data contains physiological measurements for a number of penguins from each of three species of penguin: chinstrap, gentoo, and adelie.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\nOur data frame has some columns we can’t use directly. Thus, we used the “one-hot encoding” to represent these features as binary columns.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\n\n\nThe displayed figure below shows the general relationship between sex and body mass (g) in each of the 3 penguin species. By looking at the figure, it’s clear that female penguins tend to have a lower body mass than the male penguins. In addition, the Gentoo penguins have a larger overall body mass than the other 2 species, weighing between 4000 to 6000 grams. Both the Adelie and Chinstap species weigh between 3000 to 4500 grams.\n\n\nCode\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=train, x=\"Body Mass (g)\", y=\"Sex\", hue=\"Species\")\nylab = ax.set(ylabel=\"Sex\")\n\n\n\n\n\n\n\n\nThe table below shows clear differences in flipper lengths between the 3 species. Gentoo penguins have the largest overall flipper length at 217.65mm. The Chinstrap penguins’ overall flipper length comes in at 195.46mm. Adelie penguins have the smallest overall flipper length of the 3 species, at 189.97mm. However, it’s important to note that there was much less data recorded for the Chinstrap species as compared to the other 2 species.\n\n\nCode\ntrain.groupby(['Species'])[['Flipper Length (mm)']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n\n  \n    \n      \n      Flipper Length (mm)\n    \n    \n      \n      mean\n      len\n    \n    \n      Species\n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      189.97\n      118\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      195.46\n      56\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      217.65\n      101"
  },
  {
    "objectID": "posts/blog3/index.html#cross-validation",
    "href": "posts/blog3/index.html#cross-validation",
    "title": "Classifying Palmer Penguins",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nWe will use cross-validation to get a sense for what max depth we should set for the Decision Tree Classifier.\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores = cross_val_score(best_DT, X_train[best_cols_DT], y_train, cv=5)\ncv_scores\n\n\narray([1.        , 0.94117647, 0.98039216, 0.98039216, 0.98039216])\n\n\n\n\nCode\n#code provided by Professor Phil; taken from class lecture\nmax_depth = 5\n\nwhile max_depth != 0:\n  DT = DecisionTreeClassifier(max_depth = max_depth)\n  cv_scores = cross_val_score(DT, X_train[best_cols_DT], y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Max depth = {max_depth}, score = {mean_score.round(3)}\")\n  max_depth += -1\n\n\nMax depth = 5, score = 0.98\nMax depth = 4, score = 0.98\nMax depth = 3, score = 0.976\nMax depth = 2, score = 0.945\nMax depth = 1, score = 0.731"
  },
  {
    "objectID": "posts/blog3/index.html#plotting-decision-regions",
    "href": "posts/blog3/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\n\nCode\n#Code provided by Professor Phil; taken from lecture notes.\nfrom matplotlib.patches import Patch\n\nwarnings.filterwarnings(\"ignore\")\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nPlotting decision regions for the Logistic Regression model on its 3 best chosen features: culmen length, culmen depth, and island.\n\n\nCode\nplot_regions(best_LR, X_train[best_cols_LR], y_train)\n\n\n\n\n\nPlotting decision regions for the Random Forest classifier on its 3 best chosen features: culmen length, flipper length, and island.\n\n\nCode\nplot_regions(best_RF, X_train[best_cols_RF], y_train)\n\n\n\n\n\nPlotting decision regions for the Decision Tree classifier on its 3 best chosen features: culmen length, flipper length, and island.\n\n\nCode\nplot_regions(best_DT, X_train[best_cols_DT], y_train)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, we will implement and experiment with two machine learning techniques for image compression and image segmentation.\n\n\n\n\n\n\nMay 3, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post is a reflection on Dr. Gebru’s work and her visit at Middlebury College.\n\n\n\n\n\n\nApr 16, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post fits a classifier using data from folktables and perform a bias audit for the algorithm.\n\n\n\n\n\n\nMar 31, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post that uses different machine learning models to determine the smallest number of measurements necessary to determine the species of a penguin.\n\n\n\n\n\n\nMar 15, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post that uses different machine learning models to determine the smallest number of measurements necessary to determine the species of a penguin.\n\n\n\n\n\n\nMar 10, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on optimization algorithms that are based on the gradients of functions.\n\n\n\n\n\n\nMar 2, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm. The algorithm aims to find a rule for separating distinct groups in some data.\n\n\n\n\n\n\nFeb 24, 2023\n\n\nNhi Dang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About CS 451",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "notebook/warmup.html",
    "href": "notebook/warmup.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Pclass\n      Age\n      Siblings/Spouses Aboard\n      Parents/Children Aboard\n      Fare\n      Sex_male\n    \n  \n  \n    \n      0\n      2\n      29.0\n      0\n      0\n      10.500\n      0\n    \n    \n      1\n      3\n      4.0\n      3\n      2\n      27.900\n      1\n    \n    \n      2\n      3\n      6.0\n      4\n      2\n      31.275\n      0\n    \n    \n      3\n      3\n      33.0\n      1\n      1\n      20.525\n      1\n    \n    \n      4\n      3\n      22.0\n      0\n      0\n      9.000\n      1\n    \n  \n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7968970380818053\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = int(1e3)))])\n\n\nplr = poly_LR(3)\nplr.fit(X_train, y_train)\nplr.score(X_train, y_train)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.7842031029619182\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\nplr.score(X_test, y_test).round(4)\n\n0.8258\n\n\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = plr.predict(X_test)\n\nconfusion_matrix(y_test, y_pred)\n\narray([[99, 15],\n       [16, 48]])\n\n\n\ndef compute(y_pred, y_test):\n    count_fp = 0\n    count_fn = 0\n    count_tp = 0\n    count_tn = 0\n\n    for i in range(len(y_pred)):\n        if y_pred[i] == 1 & y_test[i] == 0:\n            count_fp += 1\n        if y_pred[i] == 0 & y_test[i] == 1:\n            count_fn += 1\n        if y_pred[i] == 1 & y_test[i] == 1:\n            count_tp += 1\n        if y_pred[i] == 0 & y_test[i] == 0:\n            count_tn += 1\n        \n    fpr = count_fp / (count_fp + count_tn)\n    fnr = count_fn / (count_fn + count_tp) \n    ppv = count_tp / (count_tp + count_fp)\n    p = (count_fn + count_tp) / (count_tn + count_fp + count_fn + count_tp)\n\n    fpr_2 = (p / (1 - p)) * ((1 - ppv) / ppv) * (1 - fnr) \n    t = (fpr, fpr_2)\n    return t\n\ncompute(y_pred, y_test)\n\n(0.46261682242990654, 0.46261682242990665)\n\n\n\n#Linear regression warm up\nimport numpy as np\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\ndef predict(X, w):\n    return X@w\n\ndef score(X,y,w):\n    y_bar = y.mean()\n    y_hat = predict(X,w)\n\n    top = ((y_hat - y) ** 2 ).sum()\n    bottom = ((y_bar - y) ** 2 ).sum()\n    c = 1 - (top / bottom)\n\n    return c\n\nscore(X, y, w)\n\n0.04948727788767249\n\n\n\n#warm up 4/3\n\n# Note: this requires the ``pillow`` package to be installed\nfrom sklearn.datasets import load_sample_image\nchina = load_sample_image(\"china.jpg\")\nax = plt.axes(xticks=[], yticks=[])\nax.imshow(china);\n\n\n\n\n\ndata = china / 255.0 # use 0...1 scale\ndata = data.reshape(427 * 640, 3)\ndata.shape\n\n(273280, 3)\n\n\n\ndef plot_pixels(data, title, colors=None, N=10000):\n    if colors is None:\n        colors = data\n    \n    # choose a random subset\n    rng = np.random.RandomState(0)\n    i = rng.permutation(data.shape[0])[:N]\n    colors = colors[i]\n    R, G, B = data[i].T\n    \n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    ax[0].scatter(R, G, color=colors, marker='.')\n    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n\n    ax[1].scatter(R, B, color=colors, marker='.')\n    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n\n    fig.suptitle(title, size=20);\n\n\nplot_pixels(data, title='Input color space: 16 million possible colors')\n\n\n\n\n\nimport warnings; warnings.simplefilter('ignore')  # Fix NumPy issues.\n\nfrom sklearn.cluster import MiniBatchKMeans\nkmeans = MiniBatchKMeans(16)\nkmeans.fit(data)\nnew_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n\nplot_pixels(data, colors=new_colors,\n            title=\"Reduced color space: 16 colors\")\n\n\n\n\n\nchina_recolored = new_colors.reshape(china.shape)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6),\n                       subplot_kw=dict(xticks=[], yticks=[]))\nfig.subplots_adjust(wspace=0.05)\nax[0].imshow(china)\nax[0].set_title('Original Image', size=16)\nax[1].imshow(china_recolored)\nax[1].set_title('16-color Image', size=16);\n\n\n\n\n\n#warm up 4/5\n\nimport torch\nimport numpy as np\n\n#tensors created directly from data \ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\n#tensors created from numpy array\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.1976, 0.6343, 0.3324],\n        [0.9060, 0.5094, 0.0333]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n\n\n#warm up 4/12\n\nfrom PIL import Image\nimport urllib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nplt.gca().axis(\"off\")\n\n(-0.5, 639.5, 412.5, -0.5)\n\n\n\n\n\n\nimport numpy as np\n\ndef convolve2d(image, kernel):\n    xKern = kernel.shape[0]\n    yKern = kernel.shape[1]\n    xImg = image.shape[0]\n    yImg = image.shape[1]\n    \n    xOutput = int(((xImg - xKern + 2)) + 1)\n    yOutput = int(((yImg - yKern + 2)) + 1)\n    output = np.zeros(xOutput, yOutput)\n\n    imagePadded = image\n    \n    for y in range(image.shape[1]):\n        # Exit Convolution\n        if y > image.shape[1] - yKern:\n            break\n\n        if y % 1 == 0:\n            for x in range(image.shape[0]):\n                # Go to next row once kernel is out of bounds\n                if x > image.shape[0] - xKern:\n                    break\n                try:\n                    \n                    if x % 1 == 0:\n                        output[x, y] = (kernel * imagePadded[x: x + xKern, y: y + yKern]).sum()\n                except:\n                    break\n\n    return output\n\n\nkernel = np.array([[-1, -1, -1], \n                   [-1,  8, -1], \n                   [-1, -1, -1]])\n\nconvd = convolve2d(img, kernel)\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 8)\nplt.gca().axis(\"off\")\n\nTypeError: Cannot interpret '640' as a data type"
  },
  {
    "objectID": "notebook/classification.html",
    "href": "notebook/classification.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n\ndef training_decision_regions(model, cols, **kwargs):\n    m = model(**kwargs)\n    m.fit(np.array(X_train[cols]), y_train)\n    plot_decision_regions(np.array(X_train[cols]), y_train, clf = m)\n    ax = plt.gca()\n    ax.set(xlabel = cols[0], \n                  ylabel = cols[1], \n                  title = f\"Training accuracy = {m.score(np.array(X_train[cols]), y_train).round(2)}\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, \n              species, \n              framealpha=0.3, \n              scatterpoints=1)\n\n\ntraining_decision_regions(LogisticRegression, cols)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "notebook/end-of-course.html",
    "href": "notebook/end-of-course.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Nhi Dang"
  }
]