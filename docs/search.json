[
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog1/perceptron.py"
  },
  {
    "objectID": "posts/blog1/index.html#the-perceptron-algorithm",
    "href": "posts/blog1/index.html#the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "The Perceptron Algorithm",
    "text": "The Perceptron Algorithm\nThe algorithm updates its guesses (weights) as it runs and also keeps a record of its history of accuracy. It processes one example at a time. For a given example, it makes a prediction and checks to see if this prediction is correct. If the prediction is correct, it does nothing. Otherwise, it changes its parameters do better on this example next time around. The algorithm stops once it reaches a user-specified maximum number of steps or the accuracy is 100%."
  },
  {
    "objectID": "posts/blog1/index.html#source-code",
    "href": "posts/blog1/index.html#source-code",
    "title": "Perceptron",
    "section": "Source Code",
    "text": "Source Code\nThe algorithm has three main functions: fit(), predict(), and score().\nWe pass into fit() a feature matrix \\(X \\in R^{n \\times p}\\) and a target vector \\(y \\in R^n\\). The function updates the weights and keeps track of the history of its accuracy. Its goal is to find a good \\(\\tilde{w}\\) to separate the data points. Fit() utilizes the following function when updating its guesses:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + y_i \\tilde{x}_i\\;\\]\nScore() and predict() compares the target variable \\(y_i\\) to the predicted label \\(\\hat{y}_i^{(t)}\\). If \\(\\hat{y}^{(t)}_i y_i > 0\\), then we do nothing because the point is correctly classified. Otherwise, we perform the update on \\(\\tilde{w}\\).\n\n\nCode\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w \nprint(\"Evolution of the score over the training period:\") \nprint(p.history[-10:]) #just the last few scores\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Perceptron Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]"
  },
  {
    "objectID": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nAfter the perceptron algorithm is run, the weight vector \\(\\tilde{w}\\) generates a hyperplane that perfectly separates data points from both classes on either side of it.\n\n\nCode\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Linearly Seperable Data with Perceptron\")\nplt.savefig(\"image.jpg\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X, y)\n\n\n1.0"
  },
  {
    "objectID": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Nonconvergence of the Perceptron Algorithm",
    "text": "Nonconvergence of the Perceptron Algorithm\nWhen the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{w}\\) , but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\n\nCode\nfig = plt.scatter(X_n[:,0], X_n[:,1], c = y_n)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim(-1,3)\ntitle = plt.title(\"Nonseperable Data with Perceptron\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X_n, y_n)\n\n\n0.5"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on optimization algorithms that are based on the gradients of functions.\n\n\n\n\n\n\nMar 2, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm. The algorithm aims to find a rule for separating distinct groups in some data.\n\n\n\n\n\n\nFeb 24, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About CS 451",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "notebook/features-regularization-live.html",
    "href": "notebook/features-regularization-live.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "$$\n$$\n\n\nFeb 27, 2023"
  },
  {
    "objectID": "notebook/features-regularization-live.html#feature-maps",
    "href": "notebook/features-regularization-live.html#feature-maps",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Feature Maps",
    "text": "Feature Maps\nSuppose that we were able to extract from each point its distance from the origin. In 2d, we could take a point \\(\\mathbf{x}\\) and simply compute\n\\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nWe could then make the classification based on the value of \\(r^2\\). In this data set, it looks like the classification rule that predicts \\(1\\) if \\(r^2 < 1\\) and \\(0\\) otherwise would be a pretty good one. The important insight here is that this is also a linear model, with linear predictor function\n\\[\n\\hat{y} = \\langle \\mathbf{r}, \\mathbf{w} \\rangle\\;,\n\\]\nand predicted labels \\(\\mathbb{1}[\\hat{y} < 0]\\).\nwhere \\(\\mathbf{r}= (r^2, 1)\\) and \\(\\mathbf{w}= (1, -1)\\). This means that we can use empirical risk minimization for this problem if we just transform the features \\(\\mathbf{X}\\) first! We need to compute a matrix \\(\\mathbf{R}\\) whose \\(i\\)th row is \\(\\mathbf{r}_i = (r^2_i, 1) = (x_{i1}^2 + x_{i2}^2, 1)\\), and then use this matrix in place of \\(\\mathbf{X}\\) for our classification task.\nThe transformation \\((x_1, x_2) \\mapsto (x_1^2 + x_2^2, 1)\\) is an example of a feature map.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 A feature map \\(\\phi\\) is a function \\(\\phi:D \\rightarrow \\mathbb{R}^p\\), where \\(D\\) is the set of possible data values. If \\(d\\in D\\) is a data point, we call \\(\\phi(d) = \\mathbf{x}\\in \\mathbb{R}^p\\) the feature vector corresponding to \\(d\\). For a given feature map \\(\\phi\\), we define the map \\(\\Phi:D^n \\rightarrow \\mathbb{R}^{n\\times p}\\) as\n\\[\n\\Phi(\\mathbf{d}) = \\left(\\begin{matrix}\n     - & \\phi(d_1) & - \\\\\n     - & \\phi(d_2) & - \\\\\n     \\vdots & \\vdots & \\vdots \\\\\n     - & \\phi(d_n) & - \\\\\n\\end{matrix}\\right)\n\\]\nWe’ll often write\n\\[\n\\mathbf{X}= \\Phi(\\mathbf{d})\n\\]\nto say that \\(\\mathbf{X}\\) is the feature matrix for a data set \\(\\mathbf{d}\\).\n\n\n\nWe can think of feature maps in two ways:\nFeature maps can represent measurement processes. For example, maybe I am trying to classify penguins by species, based on physiological measurements. The real data is the penguin, and the measurements are how I represent that penguin with numbers. In this case, I might write my feature map as \\[\\phi(🐧) = (\\mathrm{height}, \\mathrm{weight}, \\text{bill length})\\] Here, \\(D\\) is a set of many penguins \\(D = \\{🐧_1, 🐧_2, 🐧_3, 🐧_4, 🐧_5, 🐧_6, 🐧_7\\}\\), and \\(d\\in D\\) is a specific penguin. The process of transforming an object into a vector via a feature map is often called vectorization as well, especially in the context of representing digital data as vectors. We often talk about vectorizing text and images for example; this can be done using feature maps.\nFeature maps can also represent data processing, which is more like our example above. There, we’re taking some data that’s already a vector and turning it into a DIFFERENT vector that we think will be helpful for our learning task."
  },
  {
    "objectID": "notebook/features-regularization-live.html#feature-maps-and-linear-separability",
    "href": "notebook/features-regularization-live.html#feature-maps-and-linear-separability",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Feature Maps and Linear Separability",
    "text": "Feature Maps and Linear Separability\nWe often think of feature maps as taking us from a space in which the data is not linearly separable to a space in which it is. For example, consider the feature map\n\\[\n(x_1, x_2) \\maps_to (x_1^2, x_2^2)\\;.\n\\]\nThis map is sufficient to express the radius information, since we can represent the radius as\n\\[\nr^2 = \\langle (1, 1), (x_1^2, x_2^2) \\rangle\\;.\n\\]\nLet’s see how this looks. We’ll again show the failed linear separator, and we’ll also show a successful separator in a transformed feature space:\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\n\nX_ = X**2\nLR2 = LogisticRegression()\nLR2.fit(X_,y)\nplot_decision_regions(X_, y, clf =  LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_,y)}\")"
  },
  {
    "objectID": "notebook/features-regularization-live.html#feature-maps-in-practice",
    "href": "notebook/features-regularization-live.html#feature-maps-in-practice",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Feature Maps in Practice",
    "text": "Feature Maps in Practice\nGoing back to our example of trying to classify the two nested circles, we could just compute the radius. In practice, however, we don’t really know which features are going to be most useful, and so we just compute a set of features. In our case, the square of the radius is an example of a polynomial of degree 2: \\[\nr^2 = x_1^2 + x_2^2\\;.\n\\] So, instead of just assuming that the radius is definitely the right thing to compute, we more frequently just compute all the monomials of degree 2 or lower. If \\(\\mathbf{x}= (x_1, x_2)\\), then this is\n\\[\n\\phi(\\mathbf{x}_i) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)\\;.\n\\]\nWe then use a linear model to solve the empirical risk minimization problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{w} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;.\n\\]\nThe important point to keep track of is that the new feature matrix \\(\\mathbf{X}' = \\Phi(\\mathbf{X})\\) has more columns than \\(\\mathbf{X}\\). In this case, for example, \\(\\mathbf{X}\\) had just 2 columns but \\(\\Phi(\\mathbf{X})\\) has 6. This means that \\(\\hat{\\mathbf{w}}\\) has 6 components, instead of 2!\nLet’s now run logistic regression with degree-2 polynomial features on this data set. The most convenient way to make this happen in the scikit-learn framework is with at Pipeline. The Pipeline first applies the feature map and then calls the model during both fitting and evaluation. We’ll wrap the pipeline in a simple function for easy reuse.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\ndef viz_plr(plr, X, y):  \n    plot_decision_regions(X, y, clf = plr)\n    score = plt.gca().set_title(f\"Accuracy = {plr.score(X, y)}\")  \n\n\n#x1, x2 space with an elipse decision boundary\n\nplr = poly_LR(degree = 2)\nplr.fit(X,y)\nviz_plr(plr,X,y)\n\n\n\n\nLet’s check the coefficients of the model:\n\n#\n\nNotice that two coefficients are much larger than the others, and approximately equal. These are the coefficients for the features \\(x_1^2\\) and \\(x_2^2\\). The fact that these are approximately equal means that our model is very close to using the square radius \\(r^2 = x_1^2 + x_2^2\\) for this data, just like we’d expect. The benefit is that we didn’t have to hard-code that in; the model just detected on its own the right pattern to find.\nPart of the reason this might be beneficial is that for some data sets, we might not really know what specific features we should try. For example, here’s another one where a linear classifier doesn’t do so great (degree 1 corresponds to no transformation of the features).\n\nnp.random.seed(123)\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\nplr = poly_LR(degree=1)\nplr.fit(X,y)\nviz_plr(plr,X,y)\n\n\n\n\nIt’s not as obvious that we should use the radius or any other specific feature for our feature map. Fortunately we don’t need to think too much about it – we can just increase the degree and let the model figure things out:\n\nplr = poly_LR(degree=5)\nplr.fit(X,y)\nviz_plr(plr,X,y)\n\n\n\n\n\nplr.named_steps[\"LR\"].coef_.round(2)\n\narray([[-0.  ,  0.14, -2.51, -2.6 , -0.83, -0.63, -0.12, -1.26,  0.2 ,\n        -0.95, -0.89, -0.57, -0.13, -0.22, -0.47,  1.48, -0.72,  0.19,\n        -0.32,  0.01, -0.5 ]])\n\n\nMuch nicer!"
  },
  {
    "objectID": "notebook/features-regularization-live.html#generalization-feature-selection-regularization",
    "href": "notebook/features-regularization-live.html#generalization-feature-selection-regularization",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Generalization, Feature Selection, Regularization",
    "text": "Generalization, Feature Selection, Regularization\nSo, why don’t we just use as many features as it takes to get perfect accuracy on the training data? Here’s an example where we get perfect accuracy on the training data:\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = 1000000)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n#model is overfitting to noise\n#effect: if we generate similar data, model won't perform well\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\n\n\n\nI’ve had to change some parameters to the LogisticRegression in order to ensure that it fully ran the optimization procedure for this many polynomials.\nThe problem here is that, although this classifier might achieve perfect training accuracy, it doesn’t really look like it’s captured “the right” pattern. This means that if we ask it to classify similar new data, it’s unlikely to do as well:\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\n\n\n\nWhoops! We have overfit: our model was so flexible that it was able to learn both some real patterns that we wanted it to learn and some noise that we didn’t. As a result, when it made a prediction on new data, the model’s predictions were imperfect, reflecting the noise it learned in the training process.\nIn machine learning practice, we don’t actually want our models to get perfect scores on the training data – we want them to generalize to new instances of unseen data. Overfitting is one way in which a model can fail to generalize.\nLet’s do an experiment in which we see what happens to the model’s generalization ability when we increase the number of polynomial features:\n\nimport pandas as pd\nnp.random.seed()\n\ndegs = range(0, 11)\n\ndf = pd.DataFrame({\"deg\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .4)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .4)\n\n    for deg in degs:\n        plr = poly_LR(degree = deg, penalty = \"none\", max_iter = 1e3)\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"deg\" : [deg],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n        \nmeans = df.groupby(\"deg\").mean().reset_index()\n\nplt.plot(means[\"deg\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"deg\"], means[\"test\"], label = \"validation\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Degree of polynomial feature\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\nInvalidParameterError: The 'max_iter' parameter of LogisticRegression must be an int in the range [0, inf). Got 1000.0 instead.\n\n\nWe observe that there is an optimal number of features for which the model is most able to generalize: around 3 or so. More features than that is actually harmful to the model’s predictive performance.\nSo, one way to promote generalization is to try to find “the right” or “the right number” of features and use them for prediction. This problem is often called feature selection.\nAnother common approach to avoid overfitting is called regularization. In regularization, we actually modify the empirical risk objective function that is to be minimized. Instead of trying to minimize Equation 1, we instead consider the modified objective function \\[\nL'(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda R(\\mathbf{w})\\;,\n\\] where \\(\\lambda\\) is a regularization strength and \\(R(\\mathbf{w})\\) is a regularization function that aims to influence the entries of \\(\\mathbf{w}\\) in some way. Common choices of regularization function include the Euclidean norm \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2\\) and the \\(\\ell_1\\) norm \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert x_j \\rvert\\). To see regularization in action, let’s go back to our logistic regression model with a large number of polynomial features. We can see the presence of overfitting in the excessive “wiggliness” of the decision boundary.\n\nX, y = make_moons(200, shuffle = True, noise = .3)\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = 1e5)\n\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\nNameError: name 'make_moons' is not defined\n\n\nFortunately for us, we can actually use regularization directly from inside the scikit-learn implementation of LogisticRegression. We specify the penalty (the \\(\\ell_1\\) regularization), the strength of the penalty (in the scikit-learn implementation, you specify \\(C = \\frac{1}{\\lambda}\\) so that larger \\(C\\) means less regularization) and the optimization solver (not all solvers work with all penalties).\n\n#\n#\n#\n\nThis looks more likely to generalize! We can also increase the regularization:\n\n#\n#\n#\n\nor decrease it:\n\n#\n#\n#\n\nLike last time, we can conduct a search (often called a grid-search) to find the best value of the regularization strength for a given problem. We’ll hold fixed the number of features, and instead vary the regularization strength:\n\nnp.random.seed()\n\nC = 10.0**np.arange(-4, 5)\n\ndf = pd.DataFrame({\"C\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .3)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .3)\n\n    for c in C:\n        plr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = c)\n\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"C\" : [c],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n     \nmeans = df.groupby(\"C\").mean().reset_index()\n\nplt.plot(means[\"C\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"C\"], means[\"test\"], label = \"validation\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"C\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\nUsing 15 features, it looks like a regularization strength of approximately \\(C = 10\\) is a good choice for this problem."
  },
  {
    "objectID": "notebook/class2_22/index.html",
    "href": "notebook/class2_22/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\nimport random \n\nalpha = 0.001;\n\ndef f(x):\n    return np.sin(x[0]*x[1])\n\ndef gradient_descent():\n   w = np.random.rand(2)\n\n   for i in range(1000):\n    w = w - alpha * gradient(w)\n\n    //np.array([   ,  ])"
  },
  {
    "objectID": "notebook/warmup/index.html",
    "href": "notebook/warmup/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import sklearn as sk\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nprint(\"I did it!\")\n\nI did it!\n\n\n\n# warmup 1: perceptron\n\ndef perceptron_classify(w, b, x):\n    return 1*(x@w - b > 0)\n    #np.dpt (w, x) == w@x\nperceptron_classify([1,2,3], 35, [4,5,6])\n\n0\n\n\n\n# warmup 2: convexity\n\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n# line 1 points\nx1 = np.linspace(-.5,1.5,100)\ny1 = -np.log(x1)\n# plotting the line 1 points \nplt.plot(x1, y1, label = \"f(z) = -ln(z)\")\n  \n# line 2 points\nx2 = np.linspace(-.5,1.5,100)\ny2 = -np.log(1-x2)\n# plotting the line 2 points\nplt.plot(x2, y2, label = \"g(z) = -ln(1-z)\")\n\n# giving a title to my graph\nplt.title('Practice Plotting')\n  \n# show a legend on the plot\nplt.legend(loc='upper right')\n  \n# function to show the plot\nplt.show()\n\n/var/folders/3v/yg1jpz7n6r9_y3hr4n2mzt_40000gn/T/ipykernel_6057/2136663577.py:8: RuntimeWarning: invalid value encountered in log\n  y1 = -np.log(x1)\n/var/folders/3v/yg1jpz7n6r9_y3hr4n2mzt_40000gn/T/ipykernel_6057/2136663577.py:14: RuntimeWarning: invalid value encountered in log\n  y2 = -np.log(1-x2)\n\n\n\n\n\n\n# warm up 3: gradient descent\n\nimport numpy as np\nimport random \n\ndef f(x):\n    return np.sin(x[0]*x[1])\n\ndef gradient_descent():\n   w = np.random.rand(2)\n   grad = np.empty(2)\n   alpha = 0.001 #learning rate\n   done = False \n   \n   while not done: \n    grad[0] = w[1] * np.cos(w[0] * w[1])\n    grad[1] = w[0] * np.cos(w[0] * w[1])\n    w = w - alpha * grad\n   \n    if np.allclose(grad, np.zeros(len(grad))):\n       done = True\n\n    return w\n   \ngradient_descent()\n\n0.1339688362183561\n\n\narray([0.24412093, 0.55043558])"
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code:"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Regular Gradient Descent",
    "text": "Accuracy of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period (last few scores):\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period (last few scores):\n[0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Regular Gradient Descent",
    "text": "Empirical Risk of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period (last few losses):\") \nprint(LR.loss_history[-10:]) #just the last few losses\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Evolution of the Loss Function\")\n\n\nEvolution of the loss over the training period (last few losses):\n[0.12706048790751118, 0.12705697777765942, 0.12705347737423786, 0.12704998666464135, 0.1270465056163979, 0.12704303419716895, 0.12703957237474778, 0.1270361201170595, 0.12703267739216018, 0.12703267739216018]"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Stochastic Gradient Descent",
    "text": "Accuracy of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period:\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Stochastic Gradient Descent",
    "text": "Empirical Risk of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period:\") \nprint(LR.loss_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Loss Function for Stochastic Gradient\")\n\n\nEvolution of the loss over the training period:\n[0.12578080459476626, 0.12577242999403068, 0.12576396545905136, 0.12575698611451894, 0.12574990651340898, 0.1257465902937433, 0.125738076528501, 0.1257353116368159, 0.1257301003301634, 0.1257301003301634]"
  },
  {
    "objectID": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Choice of Batch Size in Stochastic Gradient Descent",
    "text": "Choice of Batch Size in Stochastic Gradient Descent\nBelow is an illustration in which the choice of batch size influences how quickly the algorithm converges.\n\n\nCode\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.1, 100, 50)\n\nfig, axarr = plt.subplots(1, 2)\n\nfig = axarr[0].plot(LR.loss_history)\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\nfig = axarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n\nplt.tight_layout()"
  }
]