[
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog1/perceptron.py"
  },
  {
    "objectID": "posts/blog1/index.html#the-perceptron-algorithm",
    "href": "posts/blog1/index.html#the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "The Perceptron Algorithm",
    "text": "The Perceptron Algorithm\nThe algorithm updates its guesses (weights) as it runs and also keeps a record of its history of accuracy. It processes one example at a time. For a given example, it makes a prediction and checks to see if this prediction is correct. If the prediction is correct, it does nothing. Otherwise, it changes its parameters do better on this example next time around. The algorithm stops once it reaches a user-specified maximum number of steps or the accuracy is 100%."
  },
  {
    "objectID": "posts/blog1/index.html#source-code",
    "href": "posts/blog1/index.html#source-code",
    "title": "Perceptron",
    "section": "Source Code",
    "text": "Source Code\nThe algorithm has three main functions: fit(), predict(), and score().\nWe pass into fit() a feature matrix \\(X \\in R^{n \\times p}\\) and a target vector \\(y \\in R^n\\). The function updates the weights and keeps track of the history of its accuracy. Its goal is to find a good \\(\\tilde{w}\\) to separate the data points. Fit() utilizes the following function when updating its guesses:\n\\[\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + y_i \\tilde{x}_i\\;\\]\nScore() and predict() compares the target variable \\(y_i\\) to the predicted label \\(\\hat{y}_i^{(t)}\\). If \\(\\hat{y}^{(t)}_i y_i > 0\\), then we do nothing because the point is correctly classified. Otherwise, we perform the update on \\(\\tilde{w}\\).\n\n\nCode\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w \nprint(\"Evolution of the score over the training period:\") \nprint(p.history[-10:]) #just the last few scores\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Perceptron Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]"
  },
  {
    "objectID": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#convergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nAfter the perceptron algorithm is run, the weight vector \\(\\tilde{w}\\) generates a hyperplane that perfectly separates data points from both classes on either side of it.\n\n\nCode\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Linearly Seperable Data with Perceptron\")\nplt.savefig(\"image.jpg\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X, y)\n\n\n1.0"
  },
  {
    "objectID": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "href": "posts/blog1/index.html#nonconvergence-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Nonconvergence of the Perceptron Algorithm",
    "text": "Nonconvergence of the Perceptron Algorithm\nWhen the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{w}\\) , but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\n\nCode\nfig = plt.scatter(X_n[:,0], X_n[:,1], c = y_n)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim(-1,3)\ntitle = plt.title(\"Nonseperable Data with Perceptron\")\n\n\n\n\n\nThe score on this data is:\n\n\nCode\np.score(X_n, y_n)\n\n\n0.5"
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code: https://github.com/ndang01/ndang01.github.io/blob/main/posts/blog2/LogisticRegression.py"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Regular Gradient Descent",
    "text": "Accuracy of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period (last few scores):\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period (last few scores):\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-regular-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Regular Gradient Descent",
    "text": "Empirical Risk of Regular Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period (last few losses):\") \nprint(LR.loss_history[-10:]) #just the last few losses\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Evolution of the Loss Function\")\n\n\nEvolution of the loss over the training period (last few losses):\n[0.12756465548793788, 0.12755823771168123, 0.12755183457409008, 0.1275454460315202, 0.12753907204049475, 0.12753271255770313, 0.12752636754000055, 0.127520036944407, 0.12751372072810663, 0.12751372072810663]"
  },
  {
    "objectID": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#accuracy-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Accuracy of Stochastic Gradient Descent",
    "text": "Accuracy of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the score over the training period:\") \nprint(LR.score_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.score_history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy of Gradient Descent Algorithm\")\n\n\nEvolution of the score over the training period:\n[0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]"
  },
  {
    "objectID": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#empirical-risk-of-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Empirical Risk of Stochastic Gradient Descent",
    "text": "Empirical Risk of Stochastic Gradient Descent\n\n\nCode\nprint(\"Evolution of the loss over the training period:\") \nprint(LR.loss_history[-10:]) #just the last few scores\n\nfig = plt.plot(LR.loss_history)\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\ntitle = plt.title(\"Loss Function for Stochastic Gradient\")\n\n\nEvolution of the loss over the training period:\n[0.12482870714265093, 0.1248063720546357, 0.12478314821058926, 0.1247593290029095, 0.12473696450872014, 0.12471412006229605, 0.1246933964041914, 0.12467358666704516, 0.12465223491810985, 0.12465223491810985]"
  },
  {
    "objectID": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "href": "posts/blog2/index.html#choice-of-batch-size-in-stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "Choice of Batch Size in Stochastic Gradient Descent",
    "text": "Choice of Batch Size in Stochastic Gradient Descent\nBelow is an illustration in which the choice of batch size influences how quickly the algorithm converges.\n\n\nCode\np_features = 11\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, 0.1, 100, 50)\n\nfig, axarr = plt.subplots(1, 2)\n\nfig = axarr[0].plot(LR.loss_history)\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\nfig = axarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"image.jpg\")"
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\ndef predict(X, w):\n    return X@w\n\ndef score(X,y,w):\n    y_bar = y.mean()\n    y_hat = predict(X,w)\n\n    top = ((y_hat - y) ** 2 ).sum()\n    bottom = ((y_bar - y) ** 2 ).sum()\n    c = 1 - (top / bottom)\n\n    return c\n\nscore(X, y, w)\n\n0.07740821212096205"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on optimization algorithms that are based on the gradients of functions.\n\n\n\n\n\n\nMar 2, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm. The algorithm aims to find a rule for separating distinct groups in some data.\n\n\n\n\n\n\nFeb 24, 2023\n\n\nNhi Dang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques youâ€™ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About CS 451",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "notebook/warmup.html",
    "href": "notebook/warmup.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      Pclass\n      Age\n      Siblings/Spouses Aboard\n      Parents/Children Aboard\n      Fare\n      Sex_male\n    \n  \n  \n    \n      0\n      2\n      29.0\n      0\n      0\n      10.500\n      0\n    \n    \n      1\n      3\n      4.0\n      3\n      2\n      27.900\n      1\n    \n    \n      2\n      3\n      6.0\n      4\n      2\n      31.275\n      0\n    \n    \n      3\n      3\n      33.0\n      1\n      1\n      20.525\n      1\n    \n    \n      4\n      3\n      22.0\n      0\n      0\n      9.000\n      1\n    \n  \n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7968970380818053\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = int(1e3)))])\n\n\nplr = poly_LR(3)\nplr.fit(X_train, y_train)\nplr.score(X_train, y_train)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.7842031029619182\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\nplr.score(X_test, y_test).round(4)\n\n0.8258\n\n\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = plr.predict(X_test)\n\nconfusion_matrix(y_test, y_pred)\n\narray([[99, 15],\n       [16, 48]])\n\n\n\ndef compute(y_pred, y_test):\n    count_fp = 0\n    count_fn = 0\n    count_tp = 0\n    count_tn = 0\n\n    for i in range(len(y_pred)):\n        if y_pred[i] == 1 & y_test[i] == 0:\n            count_fp += 1\n        if y_pred[i] == 0 & y_test[i] == 1:\n            count_fn += 1\n        if y_pred[i] == 1 & y_test[i] == 1:\n            count_tp += 1\n        if y_pred[i] == 0 & y_test[i] == 0:\n            count_tn += 1\n        \n    fpr = count_fp / (count_fp + count_tn)\n    fnr = count_fn / (count_fn + count_tp) \n    ppv = count_tp / (count_tp + count_fp)\n    p = (count_fn + count_tp) / (count_tn + count_fp + count_fn + count_tp)\n\n    fpr_2 = (p / (1 - p)) * ((1 - ppv) / ppv) * (1 - fnr) \n    t = (fpr, fpr_2)\n    return t\n\ncompute(y_pred, y_test)\n\n(0.46261682242990654, 0.46261682242990665)"
  },
  {
    "objectID": "notebook/classification.html",
    "href": "notebook/classification.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows Ã— 14 columns\n\n\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n\ndef training_decision_regions(model, cols, **kwargs):\n    m = model(**kwargs)\n    m.fit(np.array(X_train[cols]), y_train)\n    plot_decision_regions(np.array(X_train[cols]), y_train, clf = m)\n    ax = plt.gca()\n    ax.set(xlabel = cols[0], \n                  ylabel = cols[1], \n                  title = f\"Training accuracy = {m.score(np.array(X_train[cols]), y_train).round(2)}\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, \n              species, \n              framealpha=0.3, \n              scatterpoints=1)\n\n\ntraining_decision_regions(LogisticRegression, cols)\n\n/Users/ndang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  }
]